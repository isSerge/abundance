
[{"content":" ðŸš§ Project Abundance ðŸš§ Researching next-gen blockchain architecture (as of 2025) to achieve ultimate scalability in permissionless setting and fully resolve Blockchain Trilemma. May or may not succeed but must be fun.\nGitHub repo Book Rust docs Zulip Chat ","date":"26 April 2025","externalUrl":null,"permalink":"/","section":"","summary":"ðŸš§ Project Abundance ðŸš§ Researching next-gen blockchain architecture (as of 2025) to achieve ultimate scalability in permissionless setting and fully resolve Blockchain Trilemma.","title":"","type":"page"},{"content":"","date":"26 April 2025","externalUrl":null,"permalink":"/authors/","section":"authors","summary":"","title":"authors","type":"authors"},{"content":"Most blockchain implementations are pieces of software that include the logic to support many different roles/features, possibly all at once: bootstrap node, block producer, RPC node, archival node, light client, etc. That is one way to do it, but one thing I learned over the years is that you can do a lot of interesting optimizations if you can apply additional constraints during the design phase.\nSo why is basically everyone trying to combine all into one? Let\u0026rsquo;s consider different roles separately first and see what is special or interesting about them.\nBootstrap node # Bootstrap node is a crucial role of P2P networks, you have to know an existing node in the network to join and software usually comes with a set of bootstrap nodes preconfigured just for this. One can use their own existing node for this or ask a friend, but from my experience, most people don\u0026rsquo;t and expect things to work out of the box without extra configuration.\nWhat does the bootstrap node need? Generally just networking and as little bandwidth as possible so it can sustain a lot of connections. What it certainly doesn\u0026rsquo;t need is running a fully-featured blockchain node, yet this is currently the only way to run Substrate-based node and that can easily cause issues, especially if some parts of the node are inefficient/unoptimized.\nBlock authoring node # Block producers in contrast to bootstrap nodes don\u0026rsquo;t need as much network connectivity, but they should be able to follow the chain, maintain transaction pool and create blocks when they have a chance according to the consensus mechanism rules. This should be done with as little latency as possibly (both in networking stack and execution) since any delays can reduce the rewards. There is no need to store or query deep history/state, and reorgs are typically only a few blocks deep. It doesn\u0026rsquo;t really need to know or do much beyond checking that blocks follow consensus rules and author the next block occasionally.\nRPC node # An RPC node can be used to query blockchain information by browser extension and other tooling, sometimes (often?) used to index the blockchain to build a block explorer (although inefficiently). In contrast to block producer, it is important to query blocks and transactions, capture various events happening on a blockchain and associate them with user transactions (e.g., to display transaction confirmation in a wallet). Depending on use case delays in block import may be less important since user may want to wait a few blocks for transaction to be confirmed anyway, so a second here or there doesn\u0026rsquo;t matter as much.\nArchival node # Archival nodes are often combined with RPC nodes to be able to query deeper history/state. However, in many blockchains they are also important for the ability to sync from genesis, which is generally considered to be the most secure (although inefficient) way to sync a blockchain node. In contrast to block authoring, there is no rush to import blocks ASAP at all. Archival node can even afford to wait for block to be reasonably deeply confirmed before even bothering to import it, which will barely compromise its functionality.\nLight client # Light client is the most special out of all mentioned so far. The goal of the light client to be \u0026ldquo;light\u0026rdquo; in terms of CPU, storage and memory requirements. In some cases, it may run in a smart contract, a very constrained environment! More likely, though, it may be a component of a wallet application, making it somewhat independent of RPC nodes and more secure and private than simply querying a centralized server. Light client typically doesn\u0026rsquo;t care about transactions at large, only monitoring a small subset of them that are of interest, otherwise simply verifying block headers to stay in sync with the network.\nWhat does this tell us? # Well, as we can see, different kinds of nodes have sometimes conflicting requirements, so trying to design a single piece of software that serves all (or most) of them will inevitably lead to inefficiencies.\nFor example, block author can probably afford to store recent data in memory and prune most of the historical data rather quickly, which leads to interesting performance optimization opportunities.\nThe archival RPC node needs to store a lot of historical information, and it needs to be efficiently retrievable, but it is not practical to fit it all into memory. Moreover, while the majority of requests will likely hit information from recent blocks, there might be older blocks of historical significance that might be queried relatively frequently, so caching might be very helpful to respond to queries quickly.\nLight clients are a completely different beast, often requiring a separate reimplementation with a different set of tradeoffs like smoldot.\nReal world # The real world is even messier than the above description. When hitting a public RPC endpoint, you may assume you\u0026rsquo;re connecting an RPC node described above. However, due to the need to serve a large number of clients, rate limit (and sometimes charge) connections/requests, you\u0026rsquo;re likely connecting to a complex geo-distributed cluster composed of many different pieces of custom software. It just happens to provide an identically looking RPC endpoint, but internals are completely different.\nIf you tried to index a blockchain through queries to RPC endpoint of an archival node, you may have noticed how painfully slow it could sometimes be, with even JSON encoding/decoding alone taking a non-negligible amount of compute. This doesn\u0026rsquo;t even account for the fact that blockchain indexing may require more detailed information than \u0026ldquo;standard\u0026rdquo; RPC can return.\nMake it a collection of libraries instead # What if there wasn\u0026rsquo;t a single implementation for all use cases? What if instead there was a collection of libraries, which can be composed into larger apps in a way that makes most sense instead?\nThis is to some degree what Substrate allows to do. In fact, Subspace does use it as a library. In contrast to most Substrate-based chains, it doesn\u0026rsquo;t use Substrate\u0026rsquo;s CLI interface at all. This allows for Subspace node implementation itself to be a library, which together with a farmer component was wrapped into a desktop application in the past, most recently into Space Acres. But even then, Substrate is not as flexible or at least not as easily usable for building blockchain client implementations with completely different requirements. It was designed to build different kinds of blockchains and serve that role successfully. However, it is often very difficult or even impossible to swap important parts and challenging to hyper-optimize due to a lot of generic APIs that any custom implementation must satisfy, whether it needs them or not (ask me how I know).\nBootstrap node implementation could pick just the networking stack and configure it in a way that supports multiple clients.\nBlock producer could include a hyper-optimized execution environment with a blazing fast database that doesn\u0026rsquo;t hit the disk very often and doesn\u0026rsquo;t need a lot of disk space. RPC node may not be needed at all.\nIndexing software can embed blockchain as a library, bypassing constraints and inefficiencies of an RPC interface. Extensible VM might allow transaction introspection with arbitrary precision (down to individual contract calls or even specific instructions) that requires much more processing power without affecting other kinds of clients. And the results can be written into a high-performance distributed database like ScyllaDB where they will actually live for production needs, possibly without maintaining historical information on the node itself.\nHaving Proof-of-Archival-Storage consensus means archival nodes are not needed to sync from genesis at all.\nLight client could combine core logic that verifies the consensus, combine it with a custom networking stack that compiles to WASM and lives in a browser extension.\nThis is what we do # We\u0026rsquo;re not building one-size-fits-all massive blockchain node implementation, but rather a collection of libraries that can be combined in various ways to satisfy a diverse set of requirements. The ability to reduce the design space for each unique use case allows writing better software. Software, which works faster, needs fewer resources and delights its users.\nThe reference implementation will offer an optimized block authoring node and likely a desktop application like Space Acres for the same purpose. There should be examples of other nodes/clients and abstractions to make them possible, but it\u0026rsquo;ll require a community/ecosystem effort to produce high-quality software that does things that people need.\nA lot of software engineering is about picking the right set of tradeoffs, and I believe this is the way to go here. If you agree or disagree and would like to discuss it, join our Zulip, I\u0026rsquo;ll be happy to chat about it.\n","date":"26 April 2025","externalUrl":null,"permalink":"/blog/2025-04-26-blockchain-as-a-library/","section":"Blog","summary":"\u003cp\u003eMost blockchain implementations are pieces of software that include the logic to support many different roles/features,\npossibly all at once: bootstrap node, block producer, RPC node, archival node, light client, etc. That is one way to do\nit, but one thing I learned over the years is that you can do a lot of interesting optimizations if you can apply\nadditional constraints during the design phase.\u003c/p\u003e\n\u003cp\u003eSo why is basically everyone trying to combine all into one? Let\u0026rsquo;s consider different roles separately first and see\nwhat is special or interesting about them.\u003c/p\u003e","title":"Blockchain as a library","type":"blog"},{"content":"","date":"26 April 2025","externalUrl":null,"permalink":"/blog/","section":"Blog","summary":"","title":"Blog","type":"blog"},{"content":"","date":"26 April 2025","externalUrl":null,"permalink":"/authors/nazar-pc/","section":"authors","summary":"","title":"nazar-pc","type":"authors"},{"content":"","date":"21 April 2025","externalUrl":null,"permalink":"/authors/adlrocha/","section":"authors","summary":"","title":"adlrocha","type":"authors"},{"content":"","date":"21 April 2025","externalUrl":null,"permalink":"/tags/consensus/","section":"tags","summary":"","title":"consensus","type":"tags"},{"content":"","date":"21 April 2025","externalUrl":null,"permalink":"/tags/status-update/","section":"tags","summary":"","title":"status-update","type":"tags"},{"content":"","date":"21 April 2025","externalUrl":null,"permalink":"/tags/","section":"tags","summary":"","title":"tags","type":"tags"},{"content":"Over the past weeks, my updates have highlighted many of the ideas emerging from our open design discussions. Now that we have a clearer direction for the design, I wanted to consolidate these ideas into a draft spec. This will serve as a foundation for implementing the first few prototypes, while also providing a structured way to gather feedback and uncover potential blind spots. I expect this spec to suffer significant changes, but it felt like the perfect way to consolidate the ideas, get feedback from the community, and unblock Nazar in case he wants to start prototyping some of the ideas we\u0026rsquo;ve been discussing.\nWhere to find the spec and the first drafts # PR192 adds a README to the specs directory including a description of all the components of the spec that have been drafted, and that I am currently working on. In this PR you can already find a first draft of the spec for the sharded archiving protocol that we\u0026rsquo;ve been discussing the past few weeks. Additionally, PR193 includes the draft for the data availability sampling protocol.\nThese drafts are not final, and may still require several rounds of feedback and iterations, so don\u0026rsquo;t expect these PRs to be immediately merged. However, you can already follow the progress through these open PRs, as well as use them to provide direct feedback about the designs. Even more, by giving us feedback in the PRs, your suggestion can be directly incorporated into the spec as we continue refining it (otherwise, you know that you have the Zulip server available for discussions and feedback).\nWhat\u0026rsquo;s next? # I decided to keep this update brief so you can focus on reading the spec and providing feedback. You will find there all the new ideas and progress from the past week. Let me hype it a bit by giving you the highlights and main changes from last week:\nInstead of using super commitments as a way to recursively commit segments to the global history of the beacon chain, we\u0026rsquo;ve simplified the protocol to allow shards, independently of their level in the hierarchy, to directly commit their segments to the global history in the beacon chain. Super segments are now created by the beacon chain to allow light clients to easily verify that shard segments belong to the global history in the beacon chain, given the right witnesses. The data availability sampling protocol specification now includes a more detailed description of the end-to-end of the protocol, including what happens if a block (or a segment) are flagged as unavailable, and how farmers in the beacon chain handle this event. This week I hope to start collecting a lot of feedback for the draft specs so I can iterate on the design and add more details and improvements where needed. In parallel, I am already figuring out how sharded plotting will work on top of sharded archiving. Fortunately, all the work Nazar has been doing to replace KZG by Merkle proofs already handles (hopefully) a lot of the heavy lifting of what will be needed for plotting.\n","date":"21 April 2025","externalUrl":null,"permalink":"/blog/2025-04-21-the-beginning-of-a-spec/","section":"Blog","summary":"\u003cp\u003eOver the past weeks, my updates have highlighted many of the ideas emerging from our open design\ndiscussions. Now that we have a clearer direction for the design, I wanted to consolidate these\nideas into a draft spec. This will serve as a foundation for implementing the first few prototypes,\nwhile also providing a structured way to gather feedback and uncover potential blind spots. I expect\nthis spec to suffer significant changes, but it felt like the perfect way to consolidate the ideas,\nget feedback from the community, and unblock Nazar in case he wants to start prototyping some of the\nideas we\u0026rsquo;ve been discussing.\u003c/p\u003e","title":"The beginning of a Spec","type":"blog"},{"content":"Last time I mentioned that I was looking into Merkle Trees to replace KZG. This week it happened, the whole codebase is basically free from KZG. The only place that is not fully fixed and where I am looking for help is GPU plotting, it broke with all these changes and isn\u0026rsquo;t the highest priority for me to fix right now.\nReplacing KZG # KZG was used for vector commitments in a few places, including archiving and plotting.\nFor archiving, it was easy: we have records with chunks to commit to, then we take commitments of all roots and create another commitment over them and include corresponding witnesses/proofs in each piece, so they can be verified against global history. Works basically the same way as before, but the proof is now larger than before.\nFor plotting, it turned out to be a bit more tricky. There is a homomorphic property that allowed farmer to extend the polynomial created over record chunks to get some parity chunks (erasure coding), but still being able to generate proofs for parity chunks that verify against original record commitment. With Merkle Trees that is no longer the case, but with both Merkle Tree creation and erasure coding (see below) being so much cheaper, we can erasure code record chunks during archiving and commit to them too. This is still fast, and when the farmer is redoing the same erasure coding later, they can generate proofs that successfully verify against record commitment. Problem solved!\nPR 175 is where KZG was completely replaced with Merkle Trees, but a lot of TODOs remained in the code for further cleanups. This alone more than doubled the archiver performance, while doing 2x erasure coding and commitments than before!\nPR 180 also added parity chunks directly to the piece, such that when doing piece verification, it isn\u0026rsquo;t necessary to do erasure coding (and heap allocations as the result). In fact, after some more changes and refactoring, PR 187 finally made it possible to use subspace-verifiction in no_std environment with no heap allocations.\nReplacing BLS12-381 erasure coding # With KZG gone, we were still doing erasure coding using BLS12-381, but we don\u0026rsquo;t have to!\nAfter looking at various options, I ended up picking reed-solomon-simd, which appears to be the fastest readily available library in Rust and has a reasonably nice public API. Unfortunately, it doesn\u0026rsquo;t yet work in no_std environment, so I sent a corresponding PR implementing that.\nErasure coding was swapped from using grandinetech/rust-kzg in PR 181, which again more than quadrupled archiving performance, wild!\nFurther archiver improvements # The usage of BLS21-381 meant we could only operate on 254-bit values, which in practice means 31-byte chunks for efficiency and simplicity purposes. This resulted in a lot of boilerplate across the codebase, introduction of RawRecord vs normal Record (first contains a multiple of 31-byte chunks vs 32-byte chunks of normal record). This was also very annoying for data retrieval since even within a single record, it wasn\u0026rsquo;t possible to simply slice the bytes. It was necessary to chunk the record into 32 byte chunks and then throw each 32nd byte away, what a mess!\nNow that BLS12-381 was completely gone from both commitments and erasure coding, it was possible to end this.\nFirst, PR 182 unified chunk size to be 32 bytes. This simplified the code a lot across the board.\nAnother issue that plagued the codebase for a long time was source/parity pieces/records interleaving. This is how polynomial extension worked in [grandinetech/rust-kzg,] and we went with it as a natural behavior, but it was another source of complexity and inefficiency, especially in data retrieval. PR 184 ended that too, streamlining the code even further.\nFinally, with refactoring in PR 185 and PR 187, the performance improved even further, while code was even easier to read.\nThe results and lessons learned # So what are the results? Multithreaded archiving time decreased from ~4.5 seconds per segment to just 0.24 seconds. This is ~18x performance improvement ðŸŽ‰. In fact, single-threaded archiving at 1.8 seconds is now faster than multithreaded before, imagine that! And there are still opportunities for performance optimizations left to explore if it ever becomes critical. This also implies much faster piece reconstruction in case it is necessary due to node sync or plotting, this was a very costly operation in the past.\nMoreover, I originally thought that the logarithmic proof size would increase the piece overhead. It turns out we\u0026rsquo;re saving a lot more by not wasting each 32nd byte of the record on padding due to KZG, so there is actually a large utilization improvement! It will actually be very handy as out proofs increase in size when we add sharding to the mix. The only place where it does use more space is the solution data structure. While unfortunate, being one per block it is not too bad.\nWhen designing Subspace we had many iterations of the design, and I think at some point we were stuck with KZG without necessarily fully taking advantage of it. There were various ideas about things like distributed archiving, which might end up benefiting from KZG, but at this point I don\u0026rsquo;t really think it is worth it. It is too slow, cumbersome to use and not post-quantum secure.\nIn retrospective, all the changes described here could be applied to Subspace if we looked back at things from the first principles after we have designed the latest version of the protocol.\nFuture improvements # Funnily enough, these are not all the breaking changes I want to do to the codebase. issue 183 describes the remaining known data retrieval challenges only discovered after mainnet launch (we didn\u0026rsquo;t really try to retrieve a lot of data during testnets) and should be addressed to greatly simplify the logic. Once done, retrieval will be much more efficient and easier to reason about (current code in Subspace is very convoluted, but unfortunately it has to be to maintain efficiency, naive retrieval is much more wasteful).\nUpcoming plans # So what is next? Well, with a bunch of technical debt dealt with, I\u0026rsquo;ll probably try to put some of the consensus logic into system contracts. All system contracts have so far managed to avoid heap allocations and making subspace-verification work without any was a stepping stone towards using it in contracts too.\nIn no so far future I\u0026rsquo;d like to have a simple single-node \u0026ldquo;blockchain\u0026rdquo; that produces blocks, while not being based on Substrate in any way. It\u0026rsquo;ll take time to get there, but progress is being made towards that.\n","date":"20 April 2025","externalUrl":null,"permalink":"/blog/2025-04-20-very-fast-archiving/","section":"Blog","summary":"\u003cp\u003eLast time I mentioned that I was looking into Merkle Trees to replace KZG. This week it happened, the whole codebase is\nbasically free from KZG. The only place that is not fully fixed and where I am looking for help is \u003ca href=\"/book/Contribute.html#gpu-plotting\"\u003eGPU plotting\u003c/a\u003e, it\nbroke with all these changes and isn\u0026rsquo;t the highest priority for me to fix right now.\u003c/p\u003e","title":"Very fast archiving","type":"blog"},{"content":"This week has been another good week of progress. I finally have a good idea of how shard archiving should work in the happy path, and I\u0026rsquo;ve started writing a low-level spec for it (that I am hoping to push to this repo soon). Unfortunately, there is still a slight gap in the spec that we need to fill before we can move forward: the data availability problem.\nCurrent state of sharded archiving design # High-level, this is how the end-to-end of sharded archiving currently looks like:\nBlock Creation (L2 Child): New blocks (blk1, blk2, etc.) are created normally in the child shard. As soon as they are created, their headers (or block hashes) are immediately committed to the parent chain (L1 child). Parent Shard Tracking (L1 Child): The parent shard keeps track in a block buffer of all the headers (or hashes) from the child shard blocks that have been committed, creating in this way an intermediate block history of all its child shards until a new segment from the shard arrives. Block History Management (L2 Child): When the history buffer in a shard accumulates enough blocks, a new segment is created in the shard. This triggers the creation of a super segment (super_segment_L2_1) which includes information about all segments and commitments added to the history and that is correspondingly submitted to the parent for commitment. It is worth nothing that super segment do not propagate the whole segment, but aggregate commitments for the segments that can be used to verify them in the upper layers of the system. Recursive operation (L1 Child): The protocol is recursive, so in the same way that the L1\u0026rsquo;s child shard was committing blocks to the parent as they were being created, it will do the same with its own parent (the beacon chain). Super-Segment Commitment (L1 Child): When a super-segment from a child (super_segment_L2_1) is committed, it is added to the shard\u0026rsquo;s history buffer along with other local segments that may have been created, triggering the clearing of the block buffer for child shards. Once a segment is committed in the parent, there is no need to keep raw blocks in the buffer anymore as they are explicitly available in the committed segments. Beacon Chain Integration (L1 Child): The beacon chain receives the blocks and segments from the immediate shards below (L1 shards), committing them to its chain (as any other regular parent shard in the system that has immediate child shards below). Super-Segment Commitment (L1 Child): When a super-segment from a child (super_segment_L1_1) is committed, it\u0026rsquo;s added to the shard\u0026rsquo;s history buffer, triggering the clearing of the block buffer. Segment History (L1 Child): Segments in a child shard are created as super-segments, committed with a list of shard and child super-segments created up to that point. Beacon Chain Role: The beacon chain commits all super-segments (segment_commitment_L1_1, super_segment_BC_1, etc.), maintaining the whole system\u0026rsquo;s historical buffer and creating a unified history. How many shards can we afford if we are committing every block? # You see in the description from the previous section that one of the key design changes since last week is that we will be committing every block to the parent to have part of the history of child shards as raw blocks until full segments for the shard are created.\nAssuming that we dedicate only a 10% of the full size of a block for the commitment of child blocks, let\u0026rsquo;s do a back-of-the-envelope calculation of the total number of child shards that we can afford. Assuming:\nBLOCK_SIZE = 4MiB BLOCK_TIME = 6 seconds BLOCK_HEADER = 224B type BlockHeader (260B) { number (8B) extrinsic_root (32B) state_root (32B) parent_hash (32B) shard_id (4B) hash (32B) solution (120B) } And that every 6 seconds all the shards in the lower level submit a block at the same time (this is the average case scenario), we can support the following number of shards in the lower level:\nNUM_SHARDS = (0.10 * BLOCK_SIZE) / BLOCK_HEADER =~ 153 SHARDS This assumes that the block time of shards is also 6 seconds and that the average case scenario happens where all shards submit their blocks at the same time on a block.\nBut maybe committing full block headers is just too much, what happens if we just commit the block (32B) hash and the shard id (4B)? Then things look a bit better, as we can get to the order of the thousand shards under each parent:\nNUM_SHARDS ~= 1111 SHARDS If we come back to our target of around 1M shards discussed last week, these numbers mean that with just two layers of shards (and the beacon chain in the root), we would be able to achieve the desired scale for the system.\nEnter the data availability problem # So far so good, we haven\u0026rsquo;t found any big blockers to the design, right? Well, we still have an elephant in the room that we need to address. While we are only committing the block headers (or hashes) to the parent, and super segments (that include an aggregate commitment for the segments in a shard) we need the raw blocks and segments to archive the history. These commitments can be used for verification, but we still need the full blocks and segments to be available in the shard when we need them. This is the data availability problem.\nFortunately, this is a problem that has been around for a while in the space (especially in the scope of Layer 2s and sharded architectures). There is a lot of literature and working implementations of projects dedicated exclusively to solving this data availability problem for other networks like Celestia or Avail.\nRoughly speaking, all the protocols proposed to solve data availability share common primitives: (i) chunking of blobs into smaller pieces that can be distributed throughout the network, (ii) erasure coding to ensure that the data can be reconstructed even if some pieces are missing, (iii) the use of vector commitments to generate proofs of possession of the data, and (iv) random sampling to verify that the data is available without having to download the entire blob, and to force holders of data to create availability proofs of the stored data.\nNext steps # This week I will be focusing on trying to specify our solution for the data availability problem so I can have a first draft of the end-to-end spec for sharded archiving. High-level, how I am thinking about this is as follows:\nWhen segments are created, they are already in an amenable format to create data availability proofs (with erasure coding and vector commitments), but block aren\u0026rsquo;t. The blocks included in the block buffer will thus have to be chunked and encoded before being committed to the parent to make them more friendly to generating availability proofs. Instead of forcing the sampling rate by protocol, nodes will be allowed to sample the availability of segments at any slot. In order to access the pieces they need to create their plots, farmers need the pieces of these segments to be available, so they are incentivised to do it periodically to ensure that they are available when needed with a high-probability (otherwise they won\u0026rsquo;t be able to create their plot until the piece is recovered). When a piece is sampled and not available, the reporting node will have to submit a transaction reporting the unavailability to the beacon chain. There are small details that need to flesh out from the sketch above, and I am planning to go through many of the data availability protocols to see if we can borrow some of their ideas. Really looking forward to sharing more of my progress next week.\n","date":"14 April 2025","externalUrl":null,"permalink":"/blog/2025-04-14-the-data-availability-problem/","section":"Blog","summary":"\u003cp\u003eThis week has been another good week of progress. I finally have a good idea of how shard archiving\nshould work in the happy path, and I\u0026rsquo;ve started writing a low-level spec for it (that I am hoping to\npush to this repo soon). Unfortunately, there is still a slight gap in the spec that we need to fill\nbefore we can move forward: the \u003cem\u003edata availability problem\u003c/em\u003e.\u003c/p\u003e","title":"The data availability problem","type":"blog"},{"content":"Last week was lighter on code changes and more heavy on research. Specifically, I\u0026rsquo;ve been looking into commitment schemes generally and Blake3 hash function in particular, which was already used in the codebase, but turns out can be applied in more interesting ways than just a hash function.\nKZG and quantum computers # There are several places in blockchains where vector commitments are used, and there are several commitment schemes that might be applicable depending on the use case. Subspace being Proof-of-Archival-Storage consensus, required a way to commit to the archival history of the blockchain among other things, where KZG commitments were used.\nKZG feels magical: it allows committing to data set with both commitment and a witness being fixed size (48 bytes) rather than logarithmic proof when Merkle Tree is used. Another neat thing is homomorphic property, which makes erasure coding of commitments equal to commitment of erasure coded data. This is unfortunately not free as KZG both require a trusted setup (the Subspace team participated in and used parameters from Ethereum\u0026rsquo;s KZG Summoning Ceremony) as well as higher compute cost when committing to data set and generating witness. The drawbacks are unfortunate but manageable.\nRecently, however, I\u0026rsquo;ve been thinking about future-proofing the design and one of the questions that popped-up is cryptography resistance to quantum computers. And sadly, KZG is not resistant to it, just like a lot of cryptography used for digital signatures (for example, when singing blockchain transactions).\nAs a result, I\u0026rsquo;ve been looking into alternatives. While there are some post-quantum schemes out there, they are not as optimized and well studied, also none of them have compact constant size commitment and proofs like in KZG. Not only that, they don\u0026rsquo;t even remotely approach proofs of Merkle Trees, the proofs are simply huge for the use case of archiving and plotting.\nSo the conclusion is, we\u0026rsquo;ll have to use Merkle Trees as a reliable, well studied and high-performance alternative.\nThis issue of quantum computers is the reason why I removed incremental archiving support in PR 168 (workaround that amortizes slow KZG commitment creation). With Merkle Trees the complexity of incremental archiving is unnecessary, so we can simplify the code a bit.\nThe topic of quantum computers will surface a few more times in the future. In the past, transaction handling was already described in a way that is agnostic to the cryptography used for transaction signing. This is also part of the reason why reward address (tied to Sr25519 signature scheme) was removed from Solution data structure in PR 169.\nBlake3 # Blake3 is a modern and very fast hash function. The reason it is fast is not only because the underlying primitives are fast, but also because it is actually based on Merkle Tree internally, rather than more common (in hash functions) Merkle-DamgÃ¥rd construction. Its design allows for both instruction-level and multi-threading parallelism.\nMost use cases probably use it as a regular hash function due to its speed, but it can also be used in a more advanced way as an actual tree! In fact, Bao project uses it for verified streaming, meaning it can verify downloaded contents as it is being downloaded without waiting for the whole file to be downloaded before hash can eb checked. Bao was originally based on already fast blake2 served as a prototype for blake3 initially, while these days it is rebased on blake3. The neat thing is that the hash of the file in Bao is the same as if the file was simply hashed with blake3!\nSo blake3 has a Merkle Tree internally, and we need Merkle Tree to commit to some data. Does this mean we can use blake3 directly instead of building a custom Merkle Tree and use blake3 as a regular hash? Turns out it is not trivial, but yes! And not only that, the cost to create such a tree is basically the hashing of the data, very nice!\nBlake3 is already used in the codebase as it was used in Subspace reference implementation in many places.\nExposing such Merkle Tree will require some low-level access to blake3 primitives and non-trivial logic, so that is delayed for now. However, PR 171 already introduced the first (of likely multiple) Merkle Tree implementation that will be upgraded to take advantage of blake3 properties later.\nOther updates # There were other minor updates, but I want to mention just two.\nA book now has Contribute page with a few topics that we are not actively working on, but would like to see contributions or collaborate on. Please join out Zulip chat to discuss if any of them are interesting, or you have something else of interest in mind.\nA second update is that PR 172 introduced a document that attempts to describe the difference with Subspace specification until the protocol has its own spec.\nUpcoming plans # The immediate next step is probably to swap the KZG commitment scheme with Merkle Tree in the archiving. There was some awkwardness in the implementation due to KZG using 254-bit scalars (or field elements or whatever they are called there), resulting in hopefully redundant abstractions. Though erasure coding will have to be replaced with a different implementation though because it is based on the same BLS12-381 curve right now.\nWe\u0026rsquo;ll see what the future holds for us soon enough, and I\u0026rsquo;ll make sure to post an update about that. See you next week!\n","date":"14 April 2025","externalUrl":null,"permalink":"/blog/2025-04-14-trees-everywhere/","section":"Blog","summary":"\u003cp\u003eLast week was lighter on code changes and more heavy on research. Specifically, I\u0026rsquo;ve been looking into commitment\nschemes generally and Blake3 hash function in particular, which was already used in the codebase, but turns out can be\napplied in more interesting ways than just a hash function.\u003c/p\u003e","title":"Trees everywhere","type":"blog"},{"content":"","date":"8 April 2025","externalUrl":null,"permalink":"/tags/announcement/","section":"tags","summary":"","title":"announcement","type":"tags"},{"content":"Welcome post mentioned briefly initial set of constraints that led to the creation of this project, but I figured it might be helpful to have a short writeup about it that might be helpful for sharing.\nIn short: we\u0026rsquo;re building a blockchain.\nBy \u0026ldquo;we\u0026rdquo; I really mean just me and Alfonso so far, but I hope more people will join over time if they find it interesting.\nA blockchain # Yes, we are simply building a blockchain. Just a blockchain, not a blockchain for AI, finance or data storage.\nAll of those things can be built on a blockchain that actually scales, but no one can predict how new technology will be used eventually, so it is important to distinguish: what the thing is from what it can be used for.\nWhy? # Blockchains started as a technology supposed to allow everyone to participate in a distributed permissionless P2P network, where and arrive at consensus for a set of state transitions without trusting anyone.\nUnfortunately, blockchains today are neither permissionless nor distributed, with a lot of trust assumptions and not scalable at all. Frustratingly, I don\u0026rsquo;t see anyone actually trying to fix all of it. Some are fixing some parts of the issue, but a comprehensive solution is lacking.\nProof-of-work that wastes computation went out of favor some time ago, partly because of energy consumption and partly because it ended up fairly centralized in practice. Proof-of-stake dominates the landscape these days but is no longer permissionless and not decentralized either. As of today, two biggest Bitcoin mining pools (Foundry USA and AntPool) have more than 50% of the hashrate, while top-5 produce more than 77%. The situation with Ethereum is not better with top-2 (Lido pool and Coinbase), resulting in more than 50% of the stake with top-5 having more than 84% of the total staked ETH. This is not the future I\u0026rsquo;m looking forward to.\nThe problem is not just that it is not decentralized, those protocols inherently can\u0026rsquo;t be decentralized. Any proof-of-work protocol that gets popular ends up centralized pools, any proof-of-stake ends up with centralized stake. I gave examples of two biggest networks above, but the same exact thing happens across the board.\nObscure languages and execution environments dominate the landscape with Solidity/EVM arguably being the biggest one. Why do we have to reinvent compilers, rewrite cryptographic libraries over and over again? There is so much software written already, wouldn\u0026rsquo;t it be better to be able to include a normal generic C library in your smart contract if it gets the job done?\nThe last big issue I have with most blockchains is that they either aren\u0026rsquo;t even trying to scale or claim to be scalable without actually being scalable. My definition of \u0026ldquo;scalable\u0026rdquo; is that the network is able to store and process more transactions as more participants join the network, without an upper bound. Ethereum\u0026rsquo;s microcontroller-like compute capabilities are not enough, Solana\u0026rsquo;s vertical scalability can\u0026rsquo;t possibly satisfy all possible demand.\nSo what? # You might agree with everything above, but wondering \u0026ldquo;so what?\u0026rdquo;\nI believe that due to countless shortcomings, many interesting applications are simply not being built, not even attempted to be built. A lot of real issues that could be solved with blockchain technology aren\u0026rsquo;t solved because of it.\nThe solution # What we\u0026rsquo;re building is a blockchain to solve all the above issues and then some.\nWe\u0026rsquo;re building a blockchain that can support literally any number of consensus participants. With Proof-of-Archival-Storage consensus, individual participants pledge disk space to store the history of the blockchain. The goal is to have a weekly payout for each terabyte of space pledged even as the blockchain gets enormously large, making pools pointless and real decentralization possible.\nWe\u0026rsquo;re building a blockchain that scales through sharding. The practical constant for max number of supported shards is expected to be one million, with no inherent limit at the protocol level. This means the ability to upload data to the blockchain at a rate of terabits per second and beyond. This also means the ability to get real compute done on chain with millions of modern high-performance CPU cores.\nWe\u0026rsquo;re building a blockchain that allows running applications written in traditional languages, with Rust being the primary target. It will be possible to debug and optimize using traditional tools like gdb and perf. With RISC-V ISA and support for standardized extensions, the code is fast and has access to modern hardware accelerators for things like cryptography without using obscure custom opcodes and VM-specific code, use high-quality high-performance libraries that already exist.\nWe\u0026rsquo;re building a blockchain that is future-proof with post-quantum cryptography.\nWe\u0026rsquo;re building a blockchain that can describe itself through metadata embedded into smart contracts, so you\u0026rsquo;ll never have to do blind signing or trust the computer when using a hardware wallet.\nIn the end, we\u0026rsquo;ll have a user-friendly blockchain that supports billions of transactions per second without compromising on security or distributed nature of the protocol.\nWhat is happening? # We\u0026rsquo;re building a blockchain already, but would love to collaborate and discuss ideas with others.\nJoin our Zulip chat for discussions and check Contribute page for the most pressing issues that are not being worked on right now, but should be.\n","date":"8 April 2025","externalUrl":null,"permalink":"/blog/2025-04-08-we-are-building-a-blockchain/","section":"Blog","summary":"\u003cp\u003e\u003ca href=\"../2025-01-13-welcome\"\u003eWelcome\u003c/a\u003e post mentioned briefly \u003ca href=\"https://gist.github.com/nazar-pc/760505c5ad7d56c20b2c75c1484e672f\" target=\"_blank\"\u003einitial set of constraints\u003c/a\u003e that led to the creation of this project, but I figured it\nmight be helpful to have a short writeup about it that might be helpful for sharing.\u003c/p\u003e\n\u003cp\u003eIn short: we\u0026rsquo;re building a blockchain.\u003c/p\u003e\n\u003cp\u003eBy \u0026ldquo;we\u0026rdquo; I really mean just me and Alfonso so far, but I hope more people will join over time if they find it\ninteresting.\u003c/p\u003e","title":"We are building a blockchain","type":"blog"},{"content":"This week I\u0026rsquo;ve gone a bit deeper into the design of the multi-shard Subspace protocol idea which I briefly introduced in my last update. The protocol is conformed by the following parts:\nSharded archiving, responsible for creating a global canonical history of the whole system, and of creating the history records that will eventually become part of farmers\u0026rsquo; plots. Sharded plotting, which takes records from the global history and seals them in plots that include segments of the history of every shard of the system, and that will be used for the farming process. And finally, merged farming, which is the protocol responsible for challenging farmer plots, and deriving the corresponding winning tickets that elect block proposers in specific shards. Let me introduce the high-level operation behind each of these sub-protocols, while digging deep in the one that I\u0026rsquo;ve focused the most on this week: sharded archiving.\nAdding the pieces together # High-level, this is how I am imagining each of these sub-protocols working together:\nFarmers self-assign themselves to whatever shards they want to farm and participate in. At this point, this is a free choice, but we will build the rewards and incentive model in a way where rational farmers will prioritise farming on shards that have less competition, thus balancing the power in the system among all shards. Farmers in every shard, independently, build the history buffer for the shard with new segments as new blocks are created and considered final in the shard. As new segments are created in a shard, their segment commitments (along with some extra metadata useful for verification) are committed to the shard\u0026rsquo;s parent so they are propagated up the hierarchy to the root (beacon chain) where the global history of the system is constructed. This history buffer in the beacon chain includes information about all segments in all shards (we will discuss briefly how this can be done in the next few sections). The way in which farmers assign themselves, i.e. show interest in participating in a shard, is by committing storage in a specific shard. So in the multi-shard version of the Subspace protocol, space is assigned exclusively to a shard, and plotting is performed in a way where sectors need to be explicitly linked to a shard. For plotting, the selection of pieces for a plot is performed globally by selecting pieces from any shard available in the global history at the beacon chain. All plots are built with pieces from all shards, and the selection of pieces is done in a way that is proportional to the amount of space assigned to each shard. So we see that plotting relies on the construction of this global history in the root of the hierarchy for its operation, but the overall mechanism is still the same as in the single-shard Subspace protocol. Finally, farming is done similarly to how is done in single-chain Subspace protocol, but with the difference that winning tickets of a plot belong exclusively to the shard where that plot is committed. I will start sharing more details on how this is done in the next few weeks, but the idea is that shards\u0026rsquo; difficulty, and thus their solution range, are independent and dynamically adjusted according to the storage committed in the shard and their historical block generation rate. Even more, we are thinking of a way to allow farmers to \u0026ldquo;merge\u0026rdquo; their plots across shards, so they can farm in multiple shards at the same time when the power in a shard is low. For this, we expect plots to be primarily committed to a specific shard, and be assigned to a set of fallback (secondary) shards so when something bad happens in one of these secondary shards, or the storage (and thus the block farmers will be allowed to generation rate) falls, farmers secondarily assigned to these shards are allowed to draw winning tickets and vote (or propose) blocks in these shards. Sharded Archiving # The operation of sharded archiving is quite straightforward, nodes in a shard are creating segments of their history that need to be submitted to the beacon chain to make them part of the global history of the system. The first question that we may ask ourselves is: can we have all shards committing segments commitments (or headers) directly into the beacon chain as soon as they are created?\nCan we commit all segment headers to the beacon chain? # Let\u0026rsquo;s a assume we have in the order of 1M shards in the system with:\nRECORDED_HISTORY_SEGMENT_SIZE = 128MiB WITNESS_SIZE = 48B COMMITMENT_SIZE = 48B BLOCK_SIZE = 4MB BLOCK_TIME = 6s This translates into:\nBLOCKS_PER_SEGMENT = SEGMENT_SIZE / BLOCK_SIZE = 32 SEGMENT_GENERATION_TIME_SHARD= (BLOCK_TIME * 32) = 192 seconds SEGMENT_RATE_PER_SLOT = 0.03 Assuming the SegmentHeader with the following struct being committed to the beacon chain:\nV0 { /// ShardID shard_id: ShardID, (20bits ~ 4Bytes) /// Segment index segment_index: SegmentIndex, (8 Bytes) /// Root of commitments of all records in a segment. segment_commitment: SegmentCommitment, (48 Bytes) /// Hash of the segment header of the previous segment prev_segment_header_hash: Blake3Hash, (32 Bytes) /// Last archived block last_archived_block: LastArchivedBlock (8 Bytes), } So if we commit each SegmentHeader we will be committing 100 bytes per segment (committing the segment header is the worst case scenario, we could optimise this and make it more compact by just committing the segment commitment with some additional metadata, but let\u0026rsquo;s consider the worst case scenario for this exercise).\nIf we get all shards committing their segment headers to the beacon chain directly instead of letting segment commitments flow up through the hierarchy, this means:\nAVG_SEGMENTS_PER_SLOT_IN_BEACON = NUM_SHARDS * 0.03 = 30k SEGMENTS / SLOT BYTES_COMMITTED_PER_SLOT_IN_BEACON = AVG_SEGMENTS_PER_SLOT_IN_BEACON * SEGMENT_HEADER_SIZE = 30 * 100 KBYTES = ~3MB With 4MiB blocks, committing all segments into the beacon chain could actually fit a single block of the beacon chain with the current rate being considered.\nThe number of lower level shards that can be supported by a parent network (and thus the beacon chain) with 4MiB blocks would be: NUM_SHARDS = BLOCK_SIZE / (SEGMENT_RATE_PER_SLOT * SEGMENT_HEADER_SIZE) = 4 / (0.03 * 100) = ~1.3M Shards This is great! It means that with a beacon chain and a single level of shards below committing their segments directly as soon as they are created we could get to the order of 1M independent shards.\nBut what if we use larger proofs? # The back-of-the-envelope calculation above assumes 48B commitments for segments, but what if we want to use a type of vector commitment for segments that is not KZG and requires bigger proofs? Then we may not be able to commit every proof, and we may need to rely on several layers of shards forming a hierarchy where parents aggregate segments to propagate them up and commit them to the global history. On top of this, we also may want to aggregate commitments from lower levels to minimise the state growth in the beacon chain, but this is a secondary concern at this point.\nI actually explored several solutions for this throughout the week:\nThe first one is a bit complex and may have been an overkill in hindsight (thank you Nazar for pointing this out). I won\u0026rsquo;t jump into the nitty gritty details and will leave this image explaining the high-level here for reference. The idea was for every parent shard to build a tree with all the segments from their children. With every new segment created in the parent chain an updated commitment for the tree of children segments is attached to the parent shard segment along with proofs for all the new segments included in the tree since the last update. In this way, parent chain segments implicitly propagate relevant information about their children to their upper layers (and eventually the beacon chain). Nazar rightly pointed out the following: why wait for parent chain segment generation as the cut-off point for committing segments? Why not batch whatever segments from the children have been submitted to a shard to propagate them to upper layers as soon as they have been included in a block? And this is the solution that I will consider for the design. The process is as follows: Let\u0026rsquo;s take as an example the tree of shards from the figure above. Shards C and D are generating new segments and submitting their commitments (or segment headers) to their parent, A. These segments are submitted to A by nodes from C and D as transactions that will be included in a block of A. As soon as A verifies a block including transactions with segments from C and D, nodes in A will create a \u0026ldquo;super segment\u0026rdquo; commitment (this is how we are going to call this data structure), that creates a commitment for all the segments included in the block of A. Nodes in A will be periodically creating their own segments and committing them to the beacon chain, as well as the super segment commitment for the segments from C and D triggered through the submission of segments in A. The super segment commitment will be a vector commitment that proves the commitment of segments in the history of A, and are used to include segments from deeper layer in batch into the global history kept in the beacon chain. Super segments will be sequenced along with regular segments from immediate children of the beacon chain conforming the global history of the system with all of the segments created in all shards. The purpose of committing these super segments from lower level shards into the beacon chain is to create the global history of the system. As soon as a super segment is committed in the beacon chain, the underlying history segments from the shards conforming this super segment are appended to the global history (implicitly sequencing the history of all shards in the global history of the beacon chain). Super segments include additional metadata with information about the segments included in them and any additional information required for verification (e.g. an aggregate commitment for the individual segments of the super segment). More details on this will be included in the spec I am currently working on. As a reference for folks with deep knowledge of the implementation of the Subspace protocol, the idea is to get a lot of inspiration to how incremental archiving currently works for the aggregation and submission of segment commitments into upper layers of the hierarchy.\nWhat\u0026rsquo;s next? # For next week I am hoping to get a first draft of the spec for sharded archiving with more lower level details of how it works (and how I imagine the implementation to be). It may be a stretch, but I am also hoping to have a good picture of how sharded plotting will look like.\nOne of the early mandates that I got from Nazar when I started was to identify any potential reasons why a system like this may not be feasible or may not reach the scale that we are aiming for, and so far I haven\u0026rsquo;t found any, which is exciting! That being said, please reach out if you see any potential flaws on my reasoning.\nHope to see you all in the next update!\n","date":"7 April 2025","externalUrl":null,"permalink":"/blog/2025-04-07-merged-farming/","section":"Blog","summary":"\u003cp\u003eThis week I\u0026rsquo;ve gone a bit deeper into the design of the multi-shard Subspace protocol idea which I briefly introduced in my last update. The protocol is conformed by the following parts:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSharded archiving, responsible for creating a global canonical history of the whole system, and of creating the history records that will eventually become part of farmers\u0026rsquo; plots.\u003c/li\u003e\n\u003cli\u003eSharded plotting, which takes records from the global history and seals them in plots that include segments of the history of every shard of the system, and that will be used for the farming process.\u003c/li\u003e\n\u003cli\u003eAnd finally, merged farming, which is the protocol responsible for challenging farmer plots, and deriving the corresponding winning tickets that elect block proposers in specific shards.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLet me introduce the high-level operation behind each of these sub-protocols, while digging deep in the one that I\u0026rsquo;ve focused the most on this week: sharded archiving.\u003c/p\u003e","title":"Merged Farming","type":"blog"},{"content":"The majority of last week I spent tinkering with Subspace codebase after importing it here in preparation for building an actual blockchain.\nAs mentioned in previous updates, transaction was the next logical step and after some preparation in PR 149 initial version landed in PR 151. It is quite basic and will likely require significant changes before used in an actual blockchain implementation, but there are just too many unknowns for now.\nOn a similar note, PR 156 added an initial version of the native token, which will be important for charging transaction fees. It is relatively simple for now, but in the spirit of what it will eventually end up being.\nWith that, Subspace codebase was imported in PR 152. This initial PR removed a bunch of features of the original codebase that will not be applicable here (domains, votes, etc.). There were many follow-up PRs that removed/refactored more things to reduce the feature set and reduce Substratisms from the codebase (PR 154, PR 155, PR 157, PR 158, PR 159). As the result the blocks can still be produced, but RPC is mostly gone (only supports what farmer needs) and many of the secondary features are gone too.\nBlockchains are hard # Now the challenge is figuring out where to go from there. Substrate is great because it gives a framework to work with, where an initial version of the blockchain may not be exactly what you want, but at least you have something working all the time. When bootstrapping from the ground up, it is easy to get lost since there are countless \u0026ldquo;chicken and egg\u0026rdquo; kinds of problems.\nThe plan right now is to first extract core consensus logic contained in the runtime into system contracts. After that to somehow get enough of node infrastructure up to get a single-node blockchain without P2P networking producing blocks.\nP2P networking for blockchain will be a major effort since while Distributed Storage Network (DSN) in Subspace is standalone, block and transaction propagation, PoT gossips are all built on top of Substrate and were never migrated off of it. Given our sharding requirements, it is likely that this (originally) DSN networking stack will have to evolve to support more things, but that is a big challenge.\nSo yeah, blockchains are hard to pull of the ground, but when building something truly different, you sometimes have to. Thankfully, some lessons were learned, and the logic already written and audited in Subspace codebase can be partially reused to speed up the process (hopefully).\nUpcoming plans # It is hard to predict how such open-ended projects will go, but one way or another I\u0026rsquo;ll be working on building the blockchain to allow us experimenting with sharding that Alfonso is designing. One thing for certain is that you can expect regular progress updates along the way.\nUntil next time ðŸ‘‹\n","date":"7 April 2025","externalUrl":null,"permalink":"/blog/2025-04-07-preparing-for-blockchain/","section":"Blog","summary":"\u003cp\u003eThe majority of last week I spent tinkering with Subspace codebase after importing it here in preparation for building\nan actual blockchain.\u003c/p\u003e","title":"Preparing for blockchain","type":"blog"},{"content":"The majority of the last two weeks I\u0026rsquo;ve been busy with the installation of the antivirus system update for my immune system. It was neither pleasant nor quick, but now that it is slowly approaching 100%, I\u0026rsquo;m back with another update of what I managed to do since the last update.\nI was able to conduct a few interviews with people of different background that helped to improve documentation (PR 134, PR 138, PR 141, PR 144). Overall good discussions, but no major issues were uncovered so far. Method context seems to be a more difficult concept to grasp that I don\u0026rsquo;t think can be avoided, but hopefully the latest revision of the book helps with understanding a bit.\nContracts as ELF files # I\u0026rsquo;ve spent a lot more time tinkering with ELF files and various options of the compiler and linker to see what would be the most ergonomic way to produce compact files without requiring user to learn some custom tooling. As a result, I came to the conclusion that a shared library is probably the best path going forward.\nContracts inherently behave like libraries that have multiple callable entrypoints, so a standard shared library is a good fit because it already contains all the necessary details about exported symbols. Another thing I really wanted to avoid is custom syscalls or other implicit \u0026ldquo;environment\u0026rdquo; for contracts to interact with. Using import symbols might be a good middle ground that avoids hardcoding a specific implementation, allowing VM to use whatever makes sense in its context.\nWith that in mind, PR 137 introduced addition CI jobs that build contracts against a custom no_std target. It is an x86-64 target for now, so I can play with its output locally, but eventually it will change to 64-bit RISC-V. While it is bare-metal-like and single-threaded, it does output a shared library in ELF format, which can be loaded with dlopen() and whose functions can be called directly. PR 136 and PR 142 introduced further tweaks to it.\nThe building process for ab-example-contract-ft in the workspace looks something like this (verbose, but completely standard):\ncargo rustc --crate-type cdylib -Z build-std=core \\ --package ab-example-contract-ft \\ --features ab-example-contract-ft/guest \\ --profile contract \\ --target x86_64-unknown-none-abundance.json Profile contract is custom and essentially enables LTO and strips symbols to produce a compact \u0026ldquo;production\u0026rdquo; output in the form of ab_example_contract_ft.contract file. -Z build-std=core will hopefully not be required at some point, similarly maybe --crate-type cdylib might be the default for the custom target somehow, so it doesn\u0026rsquo;t need to be specified explicitly.\nThe file still contains an annoying .comment section with Rust, LLVM and LLD versions, which can be stripped with:\nstrip -R .comment ab_example_contract_ft.contract I looked for ways to avoid it without this extra step, but didn\u0026rsquo;t find any.\nSo how big are the output files you might be wondering? Reasonably small actually, but ELF has relatively large 64-byte sections with a bunch of zeroes that increase the size a bit. For flipper example the size is ~1.8 kiB before manual stripping of .comment section and ~2,7 kiB for ft contract.\nThe good thing is that it compresses well with zstd, down to ~550 bytes for flipper and ~1100 bytes for ft contract (after .comment stripping), which is much closer to what I\u0026rsquo;d like to see. I think zstd compression is what will be used for contracts in practice since there is some repetition and a bunch of zeroes in the ELF file.\nThe huge benefit of this design is that it\u0026rsquo;d be possible to use all the standard tooling with these contracts. Regular Cargo/Rustc or even other compilers to produce an ELF file, no custom tooling is required (though it might be more convenient). I can also imagine a custom app that can load such a contract file plus encoded method call (dumped from block explorer, for example) to do step-by-step debugging in gdb/lldb. All standard disassembling and optimization tools should also work with these contracts.\nTransactions # I did some refactoring for transactions in preparation for a basic implementation of the transaction pool in PR 143, but ultimately didn\u0026rsquo;t have time to do anything significant there. I did spend a lot of time thinking about it and should have more progress in the coming weeks. A lot of open questions arise (like commitment schemes for things) since this is getting closer to the actual blockchain implementation (that we do not have), which is something I\u0026rsquo;ll be working on addressing in the coming weeks too.\nUpcoming plans # With basic execution environment, transactions and transaction pool, we\u0026rsquo;ll need blocks and a chain of blocks, meaning blockchain. The plan there right now is to use Subspace protocol reference implementation as the foundation initially (that I\u0026rsquo;m very familiar with). I\u0026rsquo;ll strip all the unnecessary bits there (everything related to domains, votes, probably most of the reward issuance logic, etc.) first to reduce the number of things to carry around. With that, we\u0026rsquo;ll not be using the Substrate framework either, so decoupling logic from Substrate and probably converting core consensus pieces into contracts will be the next logical step after that.\nThis will be a long-is process since Substrate provides networking stack for the blockchain, which will have to be replaced and networking stack used for DSN is not quite in the shape to replace that yet. With that, we\u0026rsquo;ll hopefully have a basic blockchain running a couple of system contracts one to two months later.\nOnce that is done, we\u0026rsquo;ll be able to compose instances of such blockchains into a sharded hierarchy and experimenting with how things fit together based on the research Alfonso is doing.\nThat is my small update for now, see you next time!\n","date":"31 March 2025","externalUrl":null,"permalink":"/blog/2025-03-31-building-contract-files/","section":"Blog","summary":"\u003cp\u003eThe majority of the last two weeks I\u0026rsquo;ve been busy with the installation of the antivirus system update for my immune\nsystem. It was neither pleasant nor quick, but now that it is slowly approaching 100%, I\u0026rsquo;m back with another update of\nwhat I managed to do since the last update.\u003c/p\u003e","title":"Building contract files","type":"blog"},{"content":"After a lot of thinking, this week I came to the realisation that a sharded architecture like the one we are trying to build can be designed leveraging the current design of the Subspace protocol and all its underlying mechanisms as a base. While this was the idea from the beginning, either for lack of familiarity with the protocol or plain ignorance, I was missing the big picture of how this could be done.\nUsing Subspace as the core protocol for our design has several advantages:\nWe can build upon the security and the robustness of the Subspace protocol. If we can assume the Subspace protocol as secure (which currently is the case), we can derive the security of our designs from the security analysis of Subspace. The team is already familiar with Subspace, this will help with its implementation and with reasoning about its correctness. We are building upon a consensus algorithm that is operating in production, and we can learn from all the improvements and mistakes made throughout its life (design and implementation-wise). Funnily, on our brainstorm sessions Nazar has already shared a few pointers to ideas that they already explored in the past and that are still relevant to us (which really helps when evaluating ideas for the design). High-level intuition for a multi-shard Subspace protocol # At this point you may be wondering already how does a multi-shard Subspace protocol would look like. I still don\u0026rsquo;t have the low-level details for the design, but at least I already have the really high-level intuition of how this could work.\nAs discussed in previous documents, ideally the protocol should be recursive and allow for infinite horizontal scaling by including deeper layers of shards The beacon chain (or main chain as referred to in some of the literature), is the main chain responsible for orchestrating the lifecycle and securing all the shards of the system. As such, all the population of farmers in the system participate from the system (see image below). By being part of the beacon chain, farmers are also implicitly proposing blocks from the underlying shards. High-level, the idea for the design is that the history buffer is populated with records belonging to the history of all shards in the system, and each of proof-of-time slot a new winning ticket will be drawn for each shard. The farmer encountering this winning ticket is responsible for sealing and broadcasting the newly proposed block for the shard they have the winning ticket for. Shards can be created in the beacon chain\u0026rsquo;s genesis, or later in the history of the chain (we will leave the specifics about this process to the future. New shards will be spin off through a network velvet forks [1]) The core idea for the system\u0026rsquo;s consensus algorithm is that consensus participants will be contributing to a global history buffer, that will be consequently archived and used for the block winner election in each shard. The beacon chain does not allow user transactions, it only accepts system transactions and block commitments from shards in the immediate layer below. Each shard has its own independent transaction pool with transactions initiated in that chain. Each Proof-of-Time (PoT) chain slot, the randomness beacon triggers a global farming process, where each farmer runs a protocol to extract a winning chunk. Ideally, there should be at least a winner for each shard. The way in which we determine if the winning ticket belongs to a specific shard is by checking to which shard the winning chunk belongs. When a farmer creates a block for a shard it broadcast the block to the corresponding shard and the header to the beacon chain. In this way, blocks in the beacon chain aggregate all the lower layer blocks that have been proposed so far. Blocks in the beacon chain are included in the history buffer, and as this blocks include the headers for the underlying shard blocks, the history buffer implicitly includes all shards blocks. The archiving is done over this global history, and when a new shard block is encountered in a genesis block, its content needs to be pulled to included in the archived history. Not all farmers need to keep the state of all shards (as it would deem the use of a sharded architecture useless). As such, every epoch (determined by a window of N slots) there is a random assignments for farmers to shards as \u0026ldquo;executors\u0026rdquo;. This assignment prevents potential collusion among farmers by keeping shard membership static. This epochs should be large enough to compensate for the \u0026ldquo;warm up\u0026rdquo; period between epoch changes where farmers may need to pull the latest state for their new shard if they don\u0026rsquo;t have it. From high-level ideas to low-level design # The high-level description shared above is great to gain an intuition of how a sharded version of the Subspace protocol could look like. Unfortunately, after a few brainstorm sessions with Nazar, we started to find holes and added complexity in how I imagined the protocol to work.\nHow can we ensure that the history buffer is populated with records belonging to the history of all shards in the system? What farmers are responsible for contributing blocks (or segments) from a shard to the history buffer if not all farmers have access to the full state of all of the shards? How are farmers assigned to specific shards and how can we prevent collusion among farmers? Which farmers are allowed to propose blocks in a specific shard? Every farmer independently of the shard? Only farmers assigned to that shard? How can we align incentives to prevent selfish farmers from just doing the least possible work to get the rewards? How can we balance the population of farmers among shards to avoid a big farmer trying to attack a shard and lead to power dilution? How should we recover from an attack in a shard? So you see that there are a lot of unanswered questions. With all of this in mind, we narrowed a bit more the design space coming up with the following ideas \u0026ndash;exploring these will be my focus on the coming week\u0026ndash;:\nInstead of having a global archiving protocol that requires every farmer to have knowledge about the state in every shard, there will be independent archiving in each shard. Shards will notify new segments to their parent chains and the beacon chain by submitting segment headers. New segments in shards are created with a local sequence ID. When the segments are committed to the beacon chain, they are assigned a global ID that sequences shard segments into a global history buffer (see figure below with a rough illustration of how this could work). Farmers are notified about new segments. If there are new pieces within their cache limit they try to go to the DHT and fetch a piece to include in their sector. Plots are tight to a specific shard. Farmers commit to farm in full branches of the hierarchical tree where they will be entitled to farm new blocks. In order to be able to do so, they\u0026rsquo;ll need to dedicate their storage to plotting on those shards (as it happens in single-chain Subspace). Along with keeping shard plots for all the shards in that branch of the tree, they will obviously also need to track the transaction pool for unverified transactions, and keep at least the most recent state for the shard. With this approach, farmers can self-assign themselves to shards, but they are required to perform some upfront work to be able to farm a block (preventing them from jumping from one shard to another with low effort). Even if they are committed to a specific branch of the tree, farmers can do light verification for other shards in the tree like validating the headers of the rest of the shards. We should introduce mechanisms, like a power threshold, used to identify when a shard is in a weak state to rebalance the population of farmers among shards. We can probably get some inspiration for this from Eigenlayer, where farmers can use their power to intervene in a shard and propose blocks to fix it. This same process should also be used to identify and recover from an attack in a shard. Finally, on top of all these mechanisms we can come up with a reward system that forces rational balancing of the farming population among shards. This will help us avoid collusion and a big farmer trying to attack a shard. Core subprotocols for the design # And I couldn\u0026rsquo;t close this weekly update without sharing a really interesting paper that Nazar brought to my attention throughout the week, and that ended up being extremely relevant to what we are doing: Scalable Multi-Chain Coordination via the Hierarchical Longest Chain Rule. This paper introduces BlockReduce, a PoW-based blockchain that achieves high-throughput by operating a hierarchy of merged mined parallel chains.\nThe paper presents a hierarchy of Nakamoto-based consensus chains like we have, and it introduces a lot of interesting concepts that reinforces or adds up to all of the ideas that we\u0026rsquo;ve been having in the past few weeks.\nThey introduce the concept of merge mining, where miners simultaneously mine multiple chains. This is similar to what we are trying to achieve with the Subspace protocol, where farmers will be able to farm multiple shards at the same time by choosing a branch of shards in the hierarchy. They propose the concept of coincident blocks, which are blocks that share the same PoW solution. This is a really interesting concept that we can use to have an implicit order of the different partitions (shards) through coincident blocks. They also propose the concept of cross-net transactions that need to be validated and can only be executed when the transactions are included in a coincident block. They use a burn-and-mint operation for cross-net transactions which really aligns with our idea of limiting cross-net transaction to the basic atomic operations. Even more, these transactions can only be executed when coincident blocks happen, as they are the ones that can attestate ordering among different shards. Finally, BlockReduce leverages a rational model, where each miner self-assigns itself to the hierarchical path that they consider more profitable for them (which is the model we are leaning towards after our latest brainstorms). What\u0026rsquo;s next? # We are starting to having a sense of how we can implement a hierarchical version of the Subspace protocol. The next step is to start breaking down the design into smaller pieces and start reasoning about their security and correctness. We need to identify what are the core subprotocols that would be needed to implement a hierarchical version of the Subspace protocol, which ones need to be adapted, and which ones can be reused unchanged.\nThis week my focus will be on trying to flesh out a first detailed spec for what I am calling the merged farming protocol, i.e. the construction of a global history buffer, a sharded archiving and plotting protocol, and sharded farming for shard block generation. On top of this, I\u0026rsquo;ll also explore alternatives for the clustering protocol, i.e. how are farmers assigned to specific shards (or partitions).\nSo with nothing else to add, see you next week!\n","date":"31 March 2025","externalUrl":null,"permalink":"/blog/2025-03-31-multi-shard-subspace-protocol/","section":"Blog","summary":"\u003cp\u003eAfter a lot of thinking, this week I came to the realisation that a sharded architecture like the one we are trying to build can be designed leveraging the current design of the Subspace protocol and all its underlying mechanisms as a base. While this was the idea from the beginning, either for lack of familiarity with the protocol or plain ignorance, I was missing the big picture of how this could be done.\u003c/p\u003e","title":"Multi-shard Subspace Protocol","type":"blog"},{"content":"Overall, I am really happy with the progress I\u0026rsquo;ve made this week. I\u0026rsquo;ve been mainly focused on unravelling one of the papers that I mentioned in my last update, Proof-of-Stake Sidechains. While is true that we don\u0026rsquo;t want our system to have anything to do with PoS, and there is no 1:1 matching of the concepts from the paper with what we are trying to build, the paper presents a framework that can come pretty handy to evaluate the correctness of our designs. This paper is from 2018, and after a first pass the first thing that I did is to check if there were any follow-up papers that built upon the abstractions of this paper. I came across Proof-of-Work Sidechains from the same authors, but without a doubt, the most complete proposal is the one that I started with. Let\u0026rsquo;s dive right into it.\nThinking in terms of sidechains. # What I liked the most about this work is that it presents a formal framework to reason about the interaction between different blockchain networks through the construction of sidechains. The paper focuses on PoS sidechains, but the concepts and the security model is general enough to be easily adaptable for other constructions like rollups, bridges, beacon chains and execution layers, or in our case, a hierarchical architecture of Nakamoto-based consensus networks.\nI would highly recommend everyone to read this paper if you are interested in the topic, but I will try to summarise the core primitives that I found more interesting and that I am planning to leverage (either for inspiration or to evaluate the correctness of our designs):\nA formalised security framework for sidechains This is whithout a doubt one of the most relevant contributions of the paper. They formalise the concept of sidechains, present formal concepts for many of the primitives that are used in the construction of sidechains, and present a security and adversarial models that can be used to reason about the security of the system. Merged staking v.s. independent staking: The paper presents two different models for the staking mechanism in sidechains. In the merged staking model, sidechains can leverage part of the staking power from the main chain to secure the network, i.e., the security of the sidechain is directly tied to the security of the main chain as some validators in the main chain also participate in the sidechain consensus. In the independent staking model, sidechains have their own staking mechanism, and their security is independent of the security of the main chain. The benefit of the merged staking model is that it helps preventing \u0026ldquo;goldfinger attacks\u0026rdquo; against sidechains with a small staking power. Direct observation v.s. certified-based observation: It also introduces two different models for the relationship between sidechain and the flow of verifiable information between them. In the direct observation model, the sidechain directly observes the main chain, i.e. it participates as a full node of the sidechain and stores and verifies every state update in that network; while in the certified-based observation model, participants of other chains do not store all the state of a sidechain, and they rely on succint proofs (i.e. certificates) to verify the state of the a sidechain. In the paper, they present what they call an ad-hoc threshold multisignatures (ATMS) construction to enable the certified-based observation model. Firewall property: The paper presents a formal definition of the firewall requirement that a sidechain must satisfy to ensure that the security of the main chain is not compromised by the sidechain. This property ensures that if a sidechain is compromised, the impact the attack can have over the main chain or other sidechains in the system is limited by the inflow of assets. The firewall property allows relying on an arbitrary definition of exactly how assets can correctly be moved back and forth between the two chains, we capture this by a so-called validity language. In case of failure, the firewall ensures that transfers from the sidechain into the main chain are rejected unless there exists a (not necessarily unique) plausible history of events on the sidechain that could, in case the sidechain was still secure, cause the particular transfers to take place. Merge operator: It also presents abstractly the use of a merge operator that allow to sequentialise a set of transactions of two independent chains. merge allows us to create a combined view of multiple ledgers, putting all of the transactions across multiple ledgers into a linear ordering. Finally, they show how their sidechain construction framework (i) supports safe cross-chain value transfers when the security assumptions of both chains are satisfied, namely that a majority of honest stake exists in both chains, and ( ii) in case of a one-sided failure, maintains the firewall property, thus containing the damage to the chains whose security conditions have been violated. What does all of this involve for design of our system? # Inspired by the concepts from above, and after doing another thorough review into Subspace\u0026rsquo;s consensus protocol (once again, thank you Nazar for clarifying the rationale behind the design of some of the parts of the protocol) I started to think about the different primitives that our design should have. These are still high-level ideas, but I am already working to flesh out the design.\nIf you recall from my previous update, our system will use an architecture composed by different layers of independent but interconnected chains (or shards), with the first layer being the global consensus (or main chain) responsible for orchestrating all the lower layers. We will adopt for our design an approach similar to the one presented for merged staking. All the farmers (we may also use consensus participants to refer to them more abstractly) dedicate their power to secure the global consensus, and they are also randomly assigned to participant and secure the consensus of the lower layers (by proposing and validating lower layer blocks). How I am planning to design this is that when a farmer audits its space to see if any of their chunks has a winning ticket to propose a block in the global consensus, they also check if they have a winning ticket to propose a block in any of the lower layers. To synchronise the different layers, I am planning to use a similar concept to that of epochs and slots. An epoch is a fixed period of time that is divided into slots. Each slot is assigned to a farmer that is responsible for proposing a block in the global consensus, and also for proposing a block in the lower layers. The global consensus will be responsible for orchestrating the assignment of slots to farmers in the lower layers, and for ensuring that the blocks proposed by the farmers in the lower layers are correctly included in the global consensus. Subspace already introduces an analogous approach through its proof-of-time that we may be able to leverage. We want Proof-of-Archival-Storage (PoAS) to apply to all the layers in the architecture, so that we can also use Proof-of-Space (PoSpace) for Sybil resistance. This is a key property that we want to maintain in our design, and one of the key things to figure out is how the protocol should work so that the history of all shards is archived and efficiently load balanced throughout the network to ensure their permanence and availability. In terms of chain observability, I expect that the lower layers will operate in a direct observation model, where they directly observe the global consensus, and they store and verify every state update in that network. We have an advantage here though, by using PoAS we make a the history of the shards available to all farmers, so that they can easily verify the state of the lower layers off-band if needed. Which brings me to the firewall property. In the paper when a sidechain fails, it is not recovering after that. The firewall property limits the impact of the attack, but there is no mechanism to make the sidechain operational again. In our case, the fact that we are archiving the history of all shards, and that we rely on a probabilistic consensus for their operation, will allow us to introduce a mechanism to recover from a failure of a shard. Thus, we want adopt the firewall property as described in the paper for our design and will try to come up with a stricter recovery property and mechanism. On top of all of this (and this is not something that I am currently considering as a high-priority) I am thinking about introducing the concept of a merge operator that allow to easily interleave the histories (or a subset of the history) between different shards. This operator will potentially be tightly coupled with the execution model that Nazar is working on, but I think that the modular approach that he is following can really help us to come up with a design that is flexible and that can be easily adapted to the different needs of the applications that will run on top of our system. Finally, the paper introduces the concepts of 2-way-pegs, cross-chain transactions, and the ability to perform atomic exchange of assets between chains. In our case, we are planning to limit the cross-shard operations to basic atomic primitives that allow to burn and mint tokens. This basic operation can be combined to construct more complex cross-chain operations by application developers. These operations will also be tightly coupled with the smart contract execution model (and already have many ideas on how to implement them), but I expect this to allow complex operation like the atomic execution of transactions involving states from two shards. In the scope of this, I had a few interesting discussions with Nazar about how addressing will work in our system, and how transactions would be queued for validations and routed throughout the hierarchy. As a side-note, I am leaving block rewards and the incentive model out of scope for this discussion for now, but I just want to let you know that we\u0026rsquo;ve also started thinking a little bit about this. What\u0026rsquo;s next? # First of all let me thank you for reading this far. This update ended up becoming a little bit longer than usual, but I wanted to share all the disconnected notes that I\u0026rsquo;ve been taking throughout the week. With all of these high-level ideas in mind, this week I am planning to focus on designing in detail the proposal of blocks in the main chain and lower level shards, and the archival of storage of the history of the shards. I think that with this we can start thinking about the implementation of the core protocol so we can surface issues or other things that need further design, while we work in parallel in detailing the rest of the system mechanics. Until next week!\n","date":"24 March 2025","externalUrl":null,"permalink":"/blog/2025-03-24-thinking-formally-in-terms-of-sidechains/","section":"Blog","summary":"\u003cp\u003eOverall, I am really happy with the progress I\u0026rsquo;ve made this week. I\u0026rsquo;ve been mainly focused on unravelling one of the\npapers that I mentioned in my last update, \u003ca href=\"https://eprint.iacr.org/2018/1239.pdf\" target=\"_blank\"\u003eProof-of-Stake Sidechains\u003c/a\u003e. While is\ntrue that we don\u0026rsquo;t want our system to have anything to do with PoS, and there is no 1:1 matching of the concepts from\nthe paper with what we are trying to build, the paper presents a framework that can come pretty handy to evaluate the\ncorrectness of our designs. This paper is from 2018, and after a first pass the first thing that I did is to check if\nthere were any follow-up papers that built upon the abstractions of this paper. I came\nacross \u003ca href=\"https://eprint.iacr.org/2018/1048.pdf\" target=\"_blank\"\u003eProof-of-Work Sidechains\u003c/a\u003e from the same authors, but without a doubt, the\nmost complete proposal is the one that I started with. Let\u0026rsquo;s dive right into it.\u003c/p\u003e","title":"Thinking formally in terms of sidechains","type":"blog"},{"content":"The plan was to get to transaction pool implementation, but it didn\u0026rsquo;t quite happen. I did a lot of investigation around performance though. For example, transaction processing was several orders of magnitude slower than direct method calls without a transaction, which concerned me, but after optimizations of last week the difference is ~10x. And it makes sense given how much more work the wallet has to do on top of the method call itself.\nSo last week we ended up with ~300 k transaction per second processing rate, while direct method calls were at ~13 M/s and transaction emulation (bypassing the wallet implementation) was at ~10.8 M/s. But why?\nThis turned out to be a combination of things, but the most impactful change was, funnily enough, probably a single stack-allocated variable that had a size 128x larger than expected. It was documented to be using byte units, but was actually expressing number of u128s. Decreasing it 128 times helped a lot. There was another data structure that was also 32 kiB in size, while needing less than one, all that and some more was corrected in PR 120.\nThat PR also introduced a constant MAX_TOTAL_METHOD_ARGS to define the maximum number of arguments the method can have. It was not constrained previously, which didn\u0026rsquo;t allow making assumptions about the max size of data structures and required heap allocations in executor. It is limited to 8, which is the number of bits in a byte and led to much anticipated rewrite of transaction payload encoding.\nThe encoding worked but was a bit less compact than I\u0026rsquo;d like it to be and didn\u0026rsquo;t support using outputs of previous method calls in the slots of the next one. Now that we know that there couldn\u0026rsquo;t be more than 8 arguments in a method, it is possible to use very compact bit flags to indicate what kind of value slot or input will use.\nPR 121 implemented the changes and fixed a few bugs. Specifically, it is now possible to create a transaction that in pseudocode looks like this:\nwallet_address = Code::deploy(wallet_code) Wallet::initialize(wallet_address, public_key) Token::send_to(wallet_address, amount) Note that in token transfer wallet_address might be a slot, and that is exactly what wasn\u0026rsquo;t supported previously. While only a simple sequence of method calls is supported by this reference transaction payload encoding, it already allows creating interesting workflows. For example, in Ethereum you often use approval in DeFi applications to allow spending your tokens by a contract. Here it would be possible to allow spending specific number of tokens just for the duration of the transaction itself:\nToken::allow_transfer_once_by(Context::Reset, contract_addres, amount) Defi::do_something(Context::Reset) Context wasn\u0026rsquo;t mentioned in examples before for simplicity, but it essentially is a notion similar to a \u0026ldquo;user\u0026rdquo; in operating systems, defining what things can be accessed. It is possible to inherit the context from a caller, override it with itself or reset to Address::NULL. Resetting means you don\u0026rsquo;t need to worry much about whatever contract you\u0026rsquo;re calling. They can\u0026rsquo;t do anything on behalf of the wallet due to context being Address::NULL already. However, by doing temporary approval, Defi contract gains an ability to do something with Token, but only within the limits of explicit approval. This is a much nicer security model that is easier to reason about IMO. And since all inputs and outputs are explicit, there is no danger of contracts messing up with wallet\u0026rsquo;s state by accident.\nSomething that Token::allow_transfer_once_by() above would use is #[tmp] argument for ephemeral storage (for duration of the transaction only), but implementation-wise it is abusing Address::NULL slots, which unfortunately were not cleaned up properly before. PR 115 finally extracted Slots data structure along with underlying aligned buffers into a separate crate, which will be needed for transaction pool implementation and PR 119 implemented cleanup for #[tmp] \u0026ldquo;slots\u0026rdquo;.\nPerformance # With transactions being much faster, I was exploring other kinds of overhead. This time the focus was on compactness and avoiding heap allocations, with PR 114, PR 124 and PR 125 performance looks roughly like this:\nflipper/direct time: [48.326 ns 48.469 ns 48.638 ns] thrpt: [20.560 Melem/s 20.632 Melem/s 20.693 Melem/s] flipper/transaction time: [57.682 ns 57.933 ns 58.219 ns] thrpt: [17.177 Melem/s 17.261 Melem/s 17.336 Melem/s] example-wallet/execute-only time: [535.60 ns 537.22 ns 539.44 ns] thrpt: [1.8538 Melem/s 1.8614 Melem/s 1.8671 Melem/s] A much smaller gap between direct method call and transaction emulation with a massive difference from last week. But more impressive is increase from ~300 k/s to 1.8 M/s for transactions that can be processed through the whole things, including the wallet. It is still bottlenecked by the signature verification cost, of course.\nThese can and will improve further. For example, metadata decoding is currently happening on every method call, but it can be cached to remove a significant amount of compute overhead from all benchmarks above.\nNo panic # There is an interesting crate in Rust ecosystem called no-panic. What it does is prevent code from compiling unless the compiler can be convinced that annotated method can\u0026rsquo;t possibly panic. This is a very nice property for assuring reliability and predictable code behavior. It is a rock-solid compiler guarantee that the API is what it appears to be. While an error handling story in Rust is already great, the fact that certain things can potentially panic is still really annoying.\nThe way it works is basically inserting a wrapper around user code that instantiates a struct that calls non-existing function in Drop implementation (that would cause linking error) before user code and removing instance after to prevent from Drop from actually running. The only way compiler would not eliminate that instance and its Drop it as dead code is if the user code can panic, in which case the Drop::drop() would need to be called during unwinding. Brilliant!\nIt is tricky to use, but with some effort can be applied to non-const (for now) methods. PR 109 implemented support for panic-free payload decoding in ab-system-contract-wallet-base and PR 118 extended this to a large portion of ab-contracts-common API. Really looking forward to trait support in const fn in Rust to be able to apply this to many const fn functions and methods in the codebase. I\u0026rsquo;ll also extend annotations to more crates over time.\nDocumentation # I need to work some more on documentation, and last week I added Transaction overview page to the book with some more details now that things have settled a bit.\nELF # I\u0026rsquo;ve spent some time researching on what format contracts should be packaged in. I thought about standard ELF rather than something custom for a long time, but didn\u0026rsquo;t know the specifics. It\u0026rsquo;d be great to use cargo build\u0026rsquo;s output, even if with custom configuration, without any manual post-processing. I\u0026rsquo;m still not 100% confident that it\u0026rsquo;ll be possible, but so far it doesn\u0026rsquo;t seem impossible either.\nFor a bit of context, what I\u0026rsquo;m ideally looking for is a 64-bit RISC-V ELF file that can be both uploaded to the blockchain as a contract and used as a \u0026ldquo;shared library\u0026rdquo; of sorts directly. By directly I mean with dlopen() or loading directly into gdb(), which will be an excellent debugging experience for developers and will eliminate the need to have custom compilers and debuggers, using industry standard tooling.\nIf it works out like I expect, it\u0026rsquo;d be possible to describe a method call in a format close to the pseudocode I have shown toward the beginning of this post and call it on a contract in gdb/lldb. Even IDE integration should work with precompiled contracts that way without the need to write special IDE-specific plugins, etc. It\u0026rsquo;ll also hopefully open the door for hardware acceleration on RISC-V hardware by running contracts in a VM, but I\u0026rsquo;m probably speculating too much at this point.\nI might start with something simpler and try to upload native x86-64 binaries in the format similar to the eventual RISC-V binaries, so I can get a feel of it and open them with dlopen() for now.\nI looked into VMs too, with PolkaVM looking interesting, but requiring PolkaVM-specific sections in ELF, which I\u0026rsquo;d really prefer to avoid. Embive looked interesting too, but has different design goals, and I\u0026rsquo;m not so sure about its performance. The impression right now is that either some PolkaVM changes would be needed to integrate it or a separate interpreter/VM will need to be designed instead.\nUpcoming plans # I\u0026rsquo;d like to get to transaction pool some time this week and to write more docs. There are also some developer interviews scheduled for later this week, which I hope will provide useful insights.\nI hope you don\u0026rsquo;t regret spending time to read this too much, see you in about a week.\n","date":"17 March 2025","externalUrl":null,"permalink":"/blog/2025-03-17-way-faster-transactions-and-no-panic/","section":"Blog","summary":"\u003cp\u003eThe plan was to get to transaction pool implementation, but it didn\u0026rsquo;t quite happen. I did a lot of investigation around\nperformance though. For example, transaction processing was several orders of magnitude slower than direct method calls\nwithout a transaction, which concerned me, but after optimizations of last week the difference is ~10x. And it makes\nsense given how much more work the wallet has to do on top of the method call itself.\u003c/p\u003e","title":"Way faster transactions and no-panic","type":"blog"},{"content":"I want to kick-off my first weekly update in the project thanking Nazar for the warm welcome and the opportunity to work with him on this exciting project. I was really pumped to see other teams actively working on a similar problem to the one I started researching more than three years ago. For several reasons, I wasn\u0026rsquo;t actively contributing to this problem any more, but this opportunity was the perfect excuse to get back to the game.\nIf you are curious about my previous work on the matter before joining Nazar, feel free to skim through this paper to get the gist of it.\nBackground # First things first, what exactly is this problem that I am referring to? At least the description of the problem is simple, \u0026ldquo;we want to design blockchain infrastructure that can scale to the size of the Internet\u0026rdquo;. The system should be able to host applications ranging from high-throughput media-intensive social networks and virtual worlds; to those that require more strict trust requirements and security guarantees, likeweb3-native and financial applications. Unfortunately, the implementation of a system like this is very challenging. Current blockchain designs are still extremely monolithic and require the execution of transactions to be performed by (almost) every node in the system, and for the state to also be replicated in each (or a great number) of them. All the innovations around L2s, rollups, and some next-gen blockchains are improving this, but no one is close to achieving a system that is able to operate at the scale of the Internet in a seamless (and if I may add, UX-friendly) way.\nThe architecture of the Internet # The best way to design a distributed system that is Internet-scale and that supports the workloads currently being executed in it is to build it from first principles, go to the source, and derive the architecture of our candidate system directly from the Internet.\nIf we look at how the Internet is structured today, we see the following layered architecture:\nA network of data centers holding the global state for all applications and the computational resources required to run applications. The interconnection of different Autonomous Systems that enables the exchange of information between different subnetworks through a routing system, logically merging all of the state and resources in the system. Local networks with a large number of devices that hang from an AS, and depend on it to interact with the rest of the network. A hierarchical DNS System that provides naming resolution through a distributed hierarchy, offering lessons for decentralised naming and discovery. How can we build a consensus algorithm that resembles this hierarchical architecture of the Internet, where there are different subnetworks that are globally orchestrated to operate as a common system? This has been the main focus for me this week, to think about how the high-level architecture of this consensus algorithm would look like considering the \u0026ldquo;wish list\u0026rdquo; set by Nazar here.\nA layered consensus # The first thing that we should acknowledge for our candidate designs is that there is a single \u0026ldquo;one-size-fits-all\u0026rdquo; consensus algorithm able to support any kind of application, but we want to come up with a design that offers the basic infrastructure that can be configured to make this possible. Depending on the application, the throughput and security guarantees are different. However, using a sound core consensus that can be tweaked and integrated as part of a bigger global consensus with the right primitives may be able to work around some of these trade-offs and achieve the desired result.\nWith this in mind, this is the high-level architecture that I\u0026rsquo;ve been tinkering with to guide my design:\nLayer 3: Local Area Consensus # Application-specific consensus run in local area subnetworks (LANs). They form dynamic clusters based on network proximity and transaction patterns Ideally, they run a consensus algorithm that allow for sub-second finality for local transactions. This consensus algorithm should be light and allow applications to configure its parameters for their needs. Provide strong local consistency with BFT guarantees (3f+1) Operate independently during network partitions Membership should be Subscription-based. They use a topic-based-like membership, where nodes that want to participate in a local are subnetwork can subscribe and unsubscribe dynamically (the subnetwork will be active as long as there are members). Proof-of-Archival should still be applied in local area networks to ensure that the history of the LAN is stored, and so we can leverage PoS for sybil resistance. The way in which I am considering this to be implemented is that LANs create fast microblocks that are broadcast to members subscribed to the LAN and the WAN (or WANs) that the LAN hangs from. Layer 2: Wide Area Consensus # Aggregates local clusters into wide-area regions Aggregate microblocks from underlying LANs and checkpoint their state through macroblocks (combining microblocks and WAN transactions). Handles cross-cluster transactions within a region Runs a Nakamoto-based consensus based on Subspace\u0026rsquo;s consensus basic primitives (i.e. employs erasure coding for data availability and Proof-of-Archival). All WANs are equal in terms of capabilities, and farmers are randomly assigned to more than one WAN (depending on the farmer population). The target block times of WANs should be similar (or better) to the ones that Autonomys currently have. WAN (as is the case for LANs) have full blockchain functionalities (transactions, smart contract executions, etc.). Layer 1: Global Consensus # Offers the higher level of security and data availability. The whole population of farmers in the system have to participate from the global consensus, as is the one orchestrating all the WANs (and implicitly LANs). There is no support for smart contract execution in the global network, and it serves exclusively as an anchor of trust, and a system-wide consensus network (with basic system functionality, like a global name system, account management, WAN farmer membership, synchrony, etc.). The global consensus run a global network analogous to the role that the beacon chain currently has in Ethereum. Provides probabilistic finality that strengthens over time. What\u0026rsquo;s next? # Obviously, many projects, from Polkadot to Optimism or Cosmos, have already realised that the best wayy to scale blockchains to support different kinds of applications is to deploy a set of subnetworks that are able to operate as a whole. So the architecture I described above doesn\u0026rsquo;t add much innovation in itself. However, I think that the key to achieve global scale is on the underlying mechanics. Using the system should be as seamless as using the Internet today.\nFor the first few strokes I will focus on designing Layers 1 and 2. This next week I want to focus on coming up with the membership protocol responsible for assigning, and the lifecycle of a transaction in the hierarchy i.e. how blocks are created, validated, and executed in the different layers, and the mechanism used to store and interleave the history of all the networks in the system.\nTo help me with this I am going to review again these papers for inspiration:\nBicomp: A Bilayer Scalable Nakamoto Consensus Protocol Close Latency-Security Trade-off for the Nakamoto Consensus Nakamoto consensus with VDFs Phantom GHOSTDAG Proof-of-Stake Sidechains (I really loved this one the first time I read it!). Narwhal and Tusk: A DAG-based mempool and efficient BFT Consensus And that concludes my first project update. I\u0026rsquo;ll try to get you a few juicy updates next week. Until then!\n","date":"16 March 2025","externalUrl":null,"permalink":"/blog/2025-03-16-drawing-inspiration-from-the-internets-architecture-to-scale-consensus/","section":"Blog","summary":"\u003cp\u003eI want to kick-off my first weekly update in the project thanking Nazar for the warm welcome and the opportunity to work with him on this exciting project. I was really pumped to see other teams actively working on a similar problem to the one I started researching more than three years ago. For several reasons, I wasn\u0026rsquo;t actively contributing to this problem any more, but this opportunity was the perfect excuse to get back to the game.\u003c/p\u003e","title":"Drawing inspiration from the Internet's architecture to scale consensus","type":"blog"},{"content":"The big change from the last update is that Alfonso de la Rocha has joined me as a part-time researcher to help with sharding designing. Code-wise, there were also a bunch of performance benchmarks and optimizations.\nAlfonso has worked extensively with blockchain-related tech and research, most recently at Protocol Labs. At PL he worked on Interplanetary Consensus (IPC) and a bunch of other things, that are all one way or another relevant to high-performance blockchains. Currently, he is learning how Subspace consensus works and prepares a framework for reasoning about sharding design options. Starting next week, there should be a section with updates from him too, so stay tuned.\nI\u0026rsquo;m very excited and I know he is too!\nTransactions # Last time I mentioned that a notion of transactions was added, but not yet integrated. I\u0026rsquo;m happy to report that it finally happened in PR 39!\nInstead of methods to manipulate environment directly, there are now dedicated methods for verification and execution of transactions using TxHandler interface that a wallet contract is supposed to implement. Crafting transactions for testing purposes might be tedious, so there is a transaction emulation API now for that purpose. NativeExecutor::env_ro() method was retained to be used for calling stateless methods, like when processing RPC requests, etc.\nExecutor creation was converted into a builder and storage container was extracted out of the executor instance, in the future it will have persistence APIs such that state can be persisted on disk and read back later. Just like you\u0026rsquo;d expect a normal blockchain node to do it, but we\u0026rsquo;re not quite there yet.\nThe same PR also introduced some test utilities in a separate crate, for example, a DummyWallet.\nPR 94, PR 100 and PR 101 updated transaction structure in preparation for work on the transaction pool, but no work on that has started yet, just some preliminary research.\nPerformance # In PR 92 I introduced benchmarking for example wallet implementation, discovering that signature verification massively dominates transaction processing time at ~14 Âµs ðŸ¤”. Not yet sure what to do with that, feels expensive, and it already takes advantage of AVX512 instructions on my Zen 4 CPU. Will have to think about that some more.\nEquipped with benchmarks, I spent a lot of time thinking and experimenting and came to the conclusion that while making multi-calls to leverage CPU concurrency is a sound idea in general, it needs to go. The reason is that instantiation cost is non-negligible and with a VM it will become even larger. At the same time, the amount of compute that can be done in a transaction in general isn\u0026rsquo;t so large that splitting it into multiple threads would be critical.\nNot having multi-calls simplified the design substantially, which after many trials and errors and long hours finally resulted in PR 103 and follow-ups PR 104 + PR 106.\nAlso learned something a bit surprising about Self and subtyping in Rust.\nThe latest numbers for execution environment for direct calls and transaction emulation overhead look like this:\nflipper/direct time: [76.389 ns 76.696 ns 77.047 ns] thrpt: [12.979 Melem/s 13.038 Melem/s 13.091 Melem/s] flipper/transaction time: [91.724 ns 91.936 ns 92.188 ns] thrpt: [10.847 Melem/s 10.877 Melem/s 10.902 Melem/s] 13 Million flips per second is A LOT more than 5 that I mentioned in previous updates, this is how much faster it became, very pleased with the results so far.\nAs for going through the while transaction processing pipeline for verification and execution of a well-formed transaction, the numbers look like this:\nexample-wallet/verify-only time: [18.268 Âµs 18.304 Âµs 18.345 Âµs] thrpt: [54.512 Kelem/s 54.634 Kelem/s 54.742 Kelem/s] example-wallet/execute-only time: [3.1408 Âµs 3.1470 Âµs 3.1545 Âµs] thrpt: [317.01 Kelem/s 317.76 Kelem/s 318.39 Kelem/s] Yeah, 54k transactions can be verified per second, while over 300k simple transactions can be executed per second. And this is all on a SINGLE CPU core. Even accounting for VM overhead, the ceiling is very high. The challenge will be to feed all these transactions at this rate (there is networking, disk access, lots of things that may slow this down).\nOther improvements # To increase the robustness of the features, PR 90 extended GitHub Actions workflows with more cases enabling various features and making sure they actually work properly.\nSince the number of crates increased, PR 98 rearranged crates into more subdirectories to help with navigation and ease discoverability of example and system contracts. Shamil will be pleased, I\u0026rsquo;m sure ðŸ˜‰. Some other of his feedback was regarding complexity and confusion regarding #[output] vs #[result]. After thinking about it and trying a few things, PR 83 and then PR 84 removed #[result], unifying the code in the process, which I hope will be not too convoluted and a bit easier to maintain. Also from him was a suggestion to support user-defined errors, which PR 82 implemented as well (with some further API improvements possible too).\nAnd a small but nice addition, PR 102 started copying original documentation (and linter attributes) from contract methods to generated extension trait methods.\nUpcoming plans # This has been a productive week, and I\u0026rsquo;m sure the next one will be too. I\u0026rsquo;m looking forward to transaction pool implementation, but not sure if I have sufficient infrastructure for that yet or this is the right time.\nI\u0026rsquo;ll be writing more documentation in the book about transactions and how they are processed now that it has solidified a bit.\nAnd I\u0026rsquo;d really like to have one or two developer interviews this week if possible to collect more feedback.\nAs usual, thank you for reading and until next time!\n","date":"9 March 2025","externalUrl":null,"permalink":"/blog/2025-03-09-there-is-two-of-us-now/","section":"Blog","summary":"\u003cp\u003eThe big change from the last update is that \u003ca href=\"https://www.linkedin.com/in/adlrocha/\" target=\"_blank\"\u003eAlfonso de la Rocha\u003c/a\u003e has joined me\nas a part-time researcher to help with sharding designing. Code-wise, there were also a bunch of performance benchmarks\nand optimizations.\u003c/p\u003e","title":"There is two of us now","type":"blog"},{"content":"The most important progress from last week is initial work on transactions. I\u0026rsquo;ve spent quite some time thinking about the design and even implemented an initial wallet contract alongside with related infrastructure.\nAs mentioned at the end of last week, adding a notion of transactions and explicit slots were the next steps and the first part of that is now implemented in PR 79, but first a bit of context.\nAs mentioned in the book, \u0026ldquo;Everything is a contract\u0026rdquo; is the design philosophy for many things, and wallets are not an exception here. This basically means that there is no \u0026ldquo;system-wide\u0026rdquo; notion way of a signature scheme to use for transactions or even a way to serialize method calls into transaction payload. The wallet is just a contract that must conform to some fairly generic interface. This interface should be flexible for all kinds of wallets: from simple ones with a public key and nonce that checks a signature and supports simple transactions to complex multisig wallets, 2FA support, support for whitelisting/blacklisting transactions depending on signer and a lot of other things I probably can\u0026rsquo;t think of right now. At the same time, contracts should compile to compact RISC-V binary and not require heap allocation in most cases, ideally taking advantage of zero-copy mechanisms whenever possible.\nAs a result, I came with a trait that looks something like this (a bit simplified for this article):\npub struct TransactionHeader { pub genesis_hash: Blake3Hash, pub block_hash: Blake3Hash, pub gas_limit: Gas, pub contract: Address, } pub type TxHandlerPayload = [u128]; pub type TxHandlerSeal = [u8]; pub trait TxHandler { /// Verify a transaction #[view] fn authorize( env: \u0026amp;Env, header: \u0026amp;TransactionHeader, payload: \u0026amp;TxHandlerPayload, seal: \u0026amp;TxHandlerSeal, ) -\u0026gt; Result\u0026lt;(), ContractError\u0026gt;; /// Execute previously verified transaction #[update] fn execute( env: \u0026amp;mut Env\u0026lt;\u0026#39;_\u0026gt;, header: \u0026amp;TransactionHeader, payload: \u0026amp;TxHandlerPayload, seal: \u0026amp;TxHandlerSeal, ) -\u0026gt; Result\u0026lt;(), ContractError\u0026gt;; } TxHandler::authorize() takes transaction header, payload and seal and must make a decision whether to authorize transaction or not. Authorization implies that the cost of gas limit will be charged before calling TxHandler::execute() and the remainder will be returned after it.\nEssentially, what we have is an interface that a node (transaction pool and execution environment) will be aware of to statelessly verify the transaction for validity and stateful way to actually execute it. The contents of a transaction payload is opaque to the execution environment (but has to be aligned to 16 bytes) just like seal is.\nThe payload canonically contains serialized method calls. Each wallet is allowed to implement it, whichever way it wants, but there are some utilities in ab-system-contract-simple-wallet-base crate/contract that provide a reference implementation of what it might look like. Specifically, that crate supports sequences of transactions and the ability to reference outputs of previous transactions in transactions that follow, which is important for contract deployment, for example.\nThe payload is aligned to the maximum alignment supported by TrivialType that is used for I/O between host and guest with inputs, this way reference serialization/deserialization of method calls ensures all data structures are correctly aligned in memory. Aligned data structures mean they don\u0026rsquo;t need to be copied, the same bytes that were received from the networking stack could be passed around as pointers and sliced into smaller data structures without allocating any more memory dynamically.\nThe seal canonically contains something that authorizes the transaction, like a cryptographic signature and nonce to prevent transaction replaying. ab-system-contract-simple-wallet-base literally has those, but one can have more than one signature, some kind of one-time token instead of nonce and all kinds of other things imaginable.\nAuthorization here is a custom code and its execution is not guaranteed to be paid for, so how is it handled?\nWell, with transaction signatures being a notion of the blockchain node, the same issue exists. So the answer here is \u0026ldquo;it depends,\u0026rdquo; specifically node should be able to configure its own limit, but once the transaction is in the block, inability to pay for it will make block invalid. It is expected that initially some low-ish limit will be set that is enough to verify afew signatures, but it may be increased over time, including by node operator. This provides the ultimate flexibility for contract developers while reducing the complexity of the node implementation.\nab-system-contract-simple-wallet-base is also deployed as a system contract, containing the foundational logic, while I also added ab-contract-example-wallet that demonstrates how to take advantage of it to have a compact and efficient wallet contract.\nHardware wallets # I\u0026rsquo;d like to dedicate a whole separate section for hardware wallets, especially in the context of recent ByBit hack.\nOne of the first things in the design of contracts was the question of how to efficiently represent data structures in serialized form. On the one hand, it is desirable to be able to pass data structures in zero-copy manner as much as possible; on the other hand, there should be a way to make sense of them. This is why I initially reached to zerocopy crate, which had tooling for this, but didn\u0026rsquo;t have metadata generation utilities like SCALE has. I also looked at musli-zerocopy, which was another promising candidate, but required a git awkward wrappers and still didn\u0026rsquo;t solve the metadata generation/parsing issue.\nIn the end, TrivialType trait was born (implemented for types that can be treated as a bunch of bytes like u8, [u32; 4], etc.) and IoType that is implemented for TrivialType and a few custom data structures. TrivialType can be derived, and derived trait will contain const METADATA. This metadata can describe all kinds of data structure shapes that can be passed between host and guest environment as \u0026ldquo;bytes,\u0026rdquo; meaning no serialization code is necessary, just a pointer to existing memory.\n#[contract] macro also implements metadata, but this time for the all methods, which includes data structures involved too. As the result, all of this information is put into ELF section to be uploaded to the blockchain together with the code and can be read by various tools.\nThere is a bunch of places that read the metadata to make sense of the data structures various methods expect. Execution environment uses it to decode data structure received from one contract and to generate another data structure when calling another. Similarly ab-system-contract-simple-wallet-base uses it to serialize/deserialize method calls to/from payload bytes.\nGoing back to the hardware wallets and blind signing that lead to the hack, it would be possible to actually both display and verify in somewhat human-readable format the contents of every transaction right on the hardware wallet itself. Especially with wallets like Ledger Stax, there is plenty of space to do so.\nThis is how it can be done:\neach transaction header contains both genesis hash and block hash for which transaction is signed genesis hash is enough to know which blockchain transaction is signed for from block hash it is possible to generate proofs about metadata of all the contracts involved in a particular transaction since hardware wallet can confirm what contract it is signing transaction for, it can also decode, display and verify its contents, for example, with utilities provided by ab-system-contract-simple-wallet-base As a result, there is no blind signing, no need to trust the UI or machine that the wallet is connected to.\nFor now the wallet would have to know which kind of wallet contract is used or else it\u0026rsquo;ll not know how to deserialize opaque payload (which is a price to pay for utmost flexibility of transaction format).\nUpcoming plans # There was a lot of preparation work and lower-level API changes done that led to the transaction interface, but I will not bother readers with it this time because the blog post is fairly large as is.\nAs mentioned in the previous update, the next big step will be to integrate this into execution environment. And there are many conveniences to add and paper-cuts to eliminate here and there, after which I\u0026rsquo;ll be looking to do more developer interviews.\nSpeaking about interviews, I had a technical interview with one of the candidates last week and hoping to have good news to share next time.\nThis was one long blog post, if you made it till the end, thank you and see you next in about one more week!\n","date":"2 March 2025","externalUrl":null,"permalink":"/blog/2025-03-02-transactions/","section":"Blog","summary":"\u003cp\u003eThe most important progress from last week is initial work on transactions. I\u0026rsquo;ve spent quite some time thinking about\nthe design and even implemented an initial wallet contract alongside with related infrastructure.\u003c/p\u003e","title":"Transactions","type":"blog"},{"content":"It was a challenging week working on storage access checks for slots, but it is over, and I\u0026rsquo;m quite happy with how things are looking right now. Some extra refactoring also allowed running tests under Miri and spotted some things that violate the Rust safety rules.\nThe work from the previous week continued on reworking the way slots are managed by native execution environment to correctly handle recursive method calls and potential access violations. It finally concluded in PR 61 with some follow-up fixes in later PRs.\nThere were several challenges with it that step from the desire to achieve high performance, while retaining efficiency and maintainability. In the end, the following rules were established: a single recursive call can modify storage, but multiple calls dispatched at once (meant to be parallel, but aren\u0026rsquo;t right now) have only read-only view. This should fit the expected use cases nicely and help to constrain code complexity. There are a few paragraphs that explain goals and results in more detail in PR 61 if you\u0026rsquo;re interested to learn more.\nI\u0026rsquo;ve been thinking about address formats some more and decided that for a global system 44 bits for addresses is really not enough, and it should be way more than that. The addresses were also stored as [u8; 8] instead of u64 to reduce alignment requirements for data structures that might contain them, so the question became what should bigger address look like and how much bigger should it really be. I then looked at RISC-V (planned to be used for a VM) assembly for different operations on byte arrays. Comparing two addresses is the most common operation here, and turned out that byte arrays comparison generates way more assembly instructions to do the same job. This is both due to RISC nature of the ISA and the fact that alignment of the byte array is 1. x86-64 has powerful instructions to read unaligned byte ranges into XMM registers and do comparison for all bytes at once, while RISC-V assembly (at least the way it is generated by rustc for riscv64imac-unknown-none-elf) was comparing bytes one pair at a time.\nAs the result, I decided that u128 will be the address format, which might be relaxed to a pair of u64s that 64-bit to reduce alignment requirement from 16 bytes to 8 (RISC-V assembly is comparing 64-bit halves separately rather than full 128-bit value at once anyway). This, landed in PR 63, which also included some refactoring for slots management, given how large a pair of addresses (owner+contract are used to identify a slot) have become.\nBased on developer interview with Shamil I have clarified and expanded on documentation in PR 64, which I hope will make it easier to understand.\nI did some initial benchmarks with PR 61, turned out it is possible to create an environment instance and call Flipper::flip on it about four million per second on a single CPU core, which gives you a good perspective of how much overhead is happening in typical blockchain environments that can only do orders of magnitude fewer simple transactions per seconds. But after slot optimizations in PR 64 I got curious if it is possible to do better and squeezed another million calls per second in PR 65.\nFive million calls per second on a single CPU core, ~200 ns per call! I\u0026rsquo;m sure it is possible to get even lower while preserving necessary logic and overall architecture. That is basically the baseline, whatever cost above that is a waste and should be minimized. perf stats look something like this:\n1 122,47 msec task-clock:u # 1,000 CPUs utilized 0 context-switches:u # 0,000 /sec 0 cpu-migrations:u # 0,000 /sec 167 page-faults:u # 148,780 /sec 5 459 682 279 cycles:u # 4,864 GHz 74 201 852 stalled-cycles-frontend:u # 1,36% frontend cycles idle 14 406 036 797 instructions:u # 2,64 insn per cycle # 0,01 stalled cycles per insn 2 470 197 650 branches:u # 2,201 G/sec 14 684 branch-misses:u # 0,00% of all branches With storage taken care of for now, there was a small problem that bothered me for a while: inability to run tests under Miri. Writing unsafe code in Rust is more challenging than in languages like C, and there is quite a bit of unsafe code due to FFI and performance reasons in the native execution environment right now. So running under Miri was very desirable, but unfortunately not possible with inventory crate that was used to make execution environment aware of all the contracts available, so implicit use of inventory had to go away.\nI still wanted to have an ergonomic API though, and that proved to be its own challenge due to the need to register both contracts themselves and traits that they implement, but traits as such aren\u0026rsquo;t types. The best thing I came up with was to instead use dyn ContractTrait as a type, but then I discovered that associated constants just like other generics make traits not object safe. I found several discussions and summarized the conclusion with some links on Rust forum. And shared in the next post an unstable (and incomplete!) feature that allows to have associated constants in traits that are object safe, but it doesn\u0026rsquo;t look likely that it\u0026rsquo;ll be stabilized any time soon. Ultimately, I had to split associated constants into a separate trait (implemented on dyn ContractTrait) and remove : Contract bound on the ContractTrait itself, but it seemed like a price worth paying. In the end, PR 66 landed a decent API that explicitly registers contracts to be used in the native execution environment (system contracts are registered internally automatically), looks something like this:\n#[test] fn basic() { let shard_index = ShardIndex::from_u32(1).unwrap(); let mut executor = NativeExecutor::in_memory_empty(shard_index) .with_contract::\u0026lt;Flipper\u0026gt;() .build() .unwrap(); // ... } That also meant tests are finally running under Miri ðŸ˜±\nYeah, Miri wasn\u0026rsquo;t too happy initially ðŸ˜…. It took a lot more reading and some help from the Rust community to figure out why, but eventually I was able to make it work in PR 67, which also added Miri tests to CI ðŸ˜Š.\nThat was the bulk of the things I got done, with some random research and WIP stuff in a local branch that I will talk about next time. Unfortunately, there were no interviews this week, but hopefully next time!\nUpcoming plans # The next steps related to execution environment will be to add a notion of a transaction. So far it was just calling methods on contracts, but the actual blockchain will have inputs serialized into a transaction. While serialization/deserialization is already happening when doing calls from contract methods, the API that developers can use externally wasn\u0026rsquo;t that. With transaction support and more explicit slots handling (ability to provide them as input and extract afterward for persistence) the workflow will be partially complete and sufficient for further interation into a bigger system with things like transaction pool. Transaction pool, of course, doesn\u0026rsquo;t exist yet (just like most other things), but it can be fixed ðŸ˜‰.\nBased on developer feedback, I would also like to simplify contract API a bit, specifically remove #[result] and make it a special case of #[output], which will remove some code duplication in execution environment and procedural macro and will be easier to explain.\nOnce those are done, I will probably conduct more developer interviews with more people (will try to hunt down some ink! maintainers or users initially). If there is someone I should definitely talk to, let me know.\nAlso, hopefully more hiring interviews this time.\nSee you in about a week with more updates!\n","date":"21 February 2025","externalUrl":null,"permalink":"/blog/2025-02-21-5-million-flips/","section":"Blog","summary":"\u003cp\u003eIt was a challenging week working on storage access checks for slots, but it is over, and I\u0026rsquo;m quite happy with how\nthings are looking right now. Some extra refactoring also allowed running tests under Miri and spotted some things that\nviolate the Rust safety rules.\u003c/p\u003e","title":"5 million flips","type":"blog"},{"content":"Last week felt a bit less productive with a lot of time spent thinking about how to approach slots conflict resolution in the native execution environment, but still managed to land a few improvements, especially on the documentation side. Also conducted four separate interviews.\nThe fist developer interview was with my past colleague Liu-Cheng, from which I\u0026rsquo;ve collected a bunch of notes. As a result, I have updated documentation on Contracts overview page in PR 57, expanding on some topics and providing more analogies for developers to connect with. Then PR 58 renamed existing \u0026ldquo;example\u0026rdquo; contract into \u0026ldquo;playground\u0026rdquo; because it was really there to try all kinds of advanced APIs that are not used in most cases, which made it hard to understand. To make things easier to comprehend, two contracts were introduced: flipper and a fungible token.\nFipper is one of the simplest contracts possible that simply flip a boolean value stored in the state to the opposite value. It doesn\u0026rsquo;t deal with slots and is there as a gentle introduction, accompanied by an integration test.\nFungible token is more advanced, supports minting and transferring tokens with balances stored in slots. It also showcases how to work with traits by implementing Fungible trait and including examples of how to use it in integration test.\nI also looked at the API provided by the execution environment and simplified a few things prior to the second developer interview with another past colleague Shamil. Shamil had a bunch more comments about various things from documentation to APIs that will take some time to incorporate into the code, but it will make things better relatively soon.\nFor example, it became clear from the interview with Shamil that the distinction between #[result], #[output] and return type is blurry, which I agree with. It\u0026rsquo;d be nice to unify all 3 into a single concept of an \u0026ldquo;output.\u0026rdquo; The primary place where it plays a significant role is #[init] method that is expected to return initial state of the contract, but I think it is possible to simply treat the last output as the state for this purpose, regardless of whether it is a return type or an explicit argument.\nAlso default return types provided by the ab-contracts-common may not be enough regardless of how extensive the selection is and developers would want to provide a custom variants that are application-specific. This might be a bit of a challenge for metadata generation, but likely worth doing anyway.\nHuge thanks to both Liu-Cheng and Shamil for spending almost 2 hours with me and going through the very early version of something completely new at the very early stages of development. The key takeaway so far is that the whole concept of \u0026ldquo;slots\u0026rdquo; is quite different from other execution environments even if justified and will be a learning curve to people. Developers typically like working with dynamic data structures whenever possible and thinking how to model the system with (ideally) fixed size slots is a slightly hostile environment. I believe the majority of it is needed for optimal performance and code size, but there is certainly room for simplification and documentation improvements. Like with fungible token example that demonstrates that one can live without a global hashmap for balances in a fairly ergonomic way.\nAs mentioned in the last update, the check for calling #[update] or #[init] from #[view] methods on execution environment level, which was implemented in PR 59. It also included a bunch of refactoring as preparation for handling of conflicting storage updates, but the handling itself wasn\u0026rsquo;t ready.\nIn fact, I have spent quite some time thinking and trying a few approaches to efficiently handle update conflicts. After many failed attempts that quickly blew up in terms of complexity, making it hard to read, and I think I found one that should work, need to do yet another attempt to implement it ðŸ¤ž.\nSpent some time looking into RISC-V specs trying to understand how it will fit into the architecture, asked a bunch of questions at PolkaVM repository. I must admit, I do not 100% like what I understood about PolkaVM so far. Not sure what the linking cost is, but my understanding right now is that linker result is what is supposed to be used as a potential contract rather than \u0026ldquo;linking\u0026rdquo; it on the fly. The problem with that is that PolkaVM linker produces something that isn\u0026rsquo;t quite vanilla RISC-V, it has some custom instructions. This isn\u0026rsquo;t inherently bad, but I imagined a system where something like a minimal ELF file is uploaded as a contract with minimal (ideally none) API that is not the standard RISC-V instruction set produced by Rust/LLVM.\nFor example, PolkaVM uses custom ecalli instruction for host function calls instead of the standard ecall instruction (which is explicitly forbidden) due to the need to do static analysis of the program, which I do not see as a requirement for this project. I\u0026rsquo;d be ideal to maybe use the same syscalls as Linux for memory allocation and add a single one for host method calls (or find something from Linux syscalls that matches). Then it would be possible to literally take a normal static C library compiled for Linux target and use it as a contract. This is not a blocker and instead more of an exploration and education for me. I hope that there will be a way to take advantage of PolkaVM in a more generic way and collaborate with them on the project.\nLastly, I had two more researcher interviews with some really nice folks, who unfortunately (for me) are not available for full-time engagement right now. So the search continues, but I believe there will be opportunities to engage them at some point of the project.\nUpcoming plans # I\u0026rsquo;m planning to finally crack the conflicting storage updates situation, further improve update documentation and, hopefully, simply API based on received developer feedback.\nAlso, hopefully more hiring interviews.\nSee you in about a week with more updates!\n","date":"14 February 2025","externalUrl":null,"permalink":"/blog/2025-02-14-initial-developer-feedback/","section":"Blog","summary":"\u003cp\u003eLast week felt a bit less productive with a lot of time spent thinking about how to approach slots conflict resolution\nin the native execution environment, but still managed to land a few improvements, especially on the documentation side.\nAlso conducted four separate interviews.\u003c/p\u003e","title":"Initial developer feedback","type":"blog"},{"content":"After a lot of refactoring and preparation, native execution environment is finally functional and can be used for purposes like writing tests and debugging.\nWhile the previous update mentioned metadata decoding for methods, turns out full metadata decoding was necessary for native execution environment and was implemented in PR 45 with some fixes and optimizations landing in PR 46 shortly afterward. One important change there was avoiding #[slot] and #[tmp] type repetition in metadata, instead it is stored in the main contract metadata right next to the state metadata, which is part of the reason why full metadata decoding was needed at this stage.\nAfter that, PR 47 made one of the last changes to FFI interface, moving size and capacity as pointers next to the data pointer in both InternalArgs and ExternalArgs for easier processing by the host. It really made things a lot simpler to handle than before and since they are pointers now, there are no issues with alignment of fields that have different types in those data structures.\nWith that, PR 50 finally introduced a native execution environment alongside some basic tests of the example contract. PR 55 further simplified things a bit, and as of right now, a contract test looks something like this. As you can see, it supports deploying system contracts first, deploying user contracts, various calls into contracts both from outside execution environment and cross-contract calls, both directly and through trait interfaces like Fungible trait for pseudo-token (implemented by example contract). A massive step forward overall!\nI mentioned InternalArgs and ExternalArgs before, but what are they? Great question indeed! With FFI interface being a bit more stable now, I have expanded contract macro documentation with details about what code it actually generates and how it is supposed to be used in PR 52. I know it is a lot of text, but it should still be a bit easier to comprehend than trying to infer what it does and why from the macros source code (even though I tried to document it as well).\nIn the process of doing that, I was more and more annoyed by the fact that there are raw pointers in Rust that can express whether the pointer can be used for writes (*mut T vs *const T) and there is a pointer that is guaranteed to be not null (NonNull), but there is no NonNullConst and NonNullMut pair or similar. Discussion on Rust Internals indicates I\u0026rsquo;m not alone, so hopefully things will improve in the future and make FFI interfaces even better.\nOutside of code changes, there was one researcher interview and there are a few more leads that will hopefully turn into more interviews later, thanks Michelle! Two developer interviews are also planned for next week; I\u0026rsquo;ll share how those went hopefully in the next update.\nUpcoming plans # While the native execution environment is there and even supports recursive calls into other contracts, it unfortunately doesn\u0026rsquo;t perform a couple of important checks. First it doesn\u0026rsquo;t reject #[update] or #[init] calls from #[view] methods, but it really should despite generated extension traits making it impossible to compile when using high-level APIs. Second, conflicting state modifications in recursive calls are not prohibited, which are trivial to do in contracts to the first issue. I\u0026rsquo;ll work on fixing these next.\nAnd interviews I\u0026rsquo;m sure will lead to some interesting feedback to reflect on. If you know someone with smart contact or blockchain development experience that would be good to talk to, let me know.\n","date":"7 February 2025","externalUrl":null,"permalink":"/blog/2025-02-07-contracts-are-actually-running/","section":"Blog","summary":"\u003cp\u003eAfter a lot of refactoring and preparation, native execution environment is finally functional and can be used for\npurposes like writing tests and debugging.\u003c/p\u003e","title":"Contracts are actually running","type":"blog"},{"content":"Last week was busy with refactoring with the primary goal of being able to run contracts in test execution environment. The environment is not quite ready yet, but a lot of progress was done, and it\u0026rsquo;ll hopefully be ready next week.\nThe plan is to have several execution environments. Naturally, the blockchain environment will run a VM with gas metering, etc. But it is less convenient to debug, which is why test/native execution environment will be available as well that can run things exactly the same way, but with access to usual debugging and other tools. The data flow will still happen through the same FFI functions as guest environment in a VM, which means there will be as few actual code path differences as possible.\nTo improve certainly in code, especially because there is a decent amount of unsafe, some of which is auto-generated, safety is important and Miri is a great tool for this purpose. Since execution environment is generic and doesn\u0026rsquo;t really know types it deals with at compile time, I initiated a discussion on Rust forum to make sure Strict Provenance is possible even in this unusual situation.\nContract and method metadata in particular was introduced a while ago to describe to the host and to avoid higher-level tools what contract contains in terms of its FFI surface, but it was far from complete.\nPR 35 addressed function fingerprint only having a non-functional stub implementation. Function signature is somewhat similar to function selector in EVM, except it uses cryptographically secure hashing function (due to the need of being able to compute in const function const-sha1 crate is used, but I also opened a PR a few weeks ago to make blake3 work in const functions too, but it is not merged yet) and is supposed to uniquely represent signature of a method when making external method calls (call will fail in case of fingerprint mismatch).\nMetadata compaction was also implemented as part of PR 35 that strips data structures of unnecessary details. This allows implementations to change argument, field and data structure names as long as the shape of the data is exactly the same, without affecting fingerprint.\nGetting closer to execution environment implementation and striving to great developer experience, storing of function pointers was introduced in PR 40 (and improved in PR 42), which implements a global registry of all methods provided by all contracts that are being linked into the binary using linkme crate. This means that no explicit actions are necessary for developers beyond adding crate to dependencies, which they would have to do anyway to be able to deploy a contracts, access its helper methods, etc. I do not like that it is so implicit too much, but I think the context and usability win justifies it.\nTo be able to dynamically and \u0026ldquo;manually\u0026rdquo; construct data structures of correct shape, it is necessary to process metadata in various ways, which is tedious and error-prone. PR 43 introduced utilities to decode method metadata and return something that is more convenient to use for creating and reading internal (host â†’ guest) and external (guest â†’ host) data structures properly. It also verifies metadata as it processes it, rejecting invalid contents (which is another \u0026ldquo;free\u0026rdquo; test case for metadata generation).\nWith those and some more minor improvements and refactoring (which as always, you\u0026rsquo;re free to check out in individual PRs), native execution is almost here. A bit more work still remains that will have to wait until the next update before contracts are actually running for real.\nUpcoming plans # Just like last time, the plan is to work on execution environment to be able to run contracts together and have a better understanding of what it feels like as a whole. Once that is done, I\u0026rsquo;ll be doing developer interviews collecting unfiltered feedback from other developers about the system before moving much further. It is important to understand if anyone actually wants something like this or not before investing too much effort into it.\nAdditionally, interviews will start with potential candidates that might help with sharding design research, which I could use a lot of help with, especially with math.\n","date":"30 January 2025","externalUrl":null,"permalink":"/blog/2025-01-30-contracts-are-almost-running/","section":"Blog","summary":"\u003cp\u003eLast week was busy with refactoring with the primary goal of being able to run contracts in test execution environment.\nThe environment is not quite ready yet, but a lot of progress was done, and it\u0026rsquo;ll hopefully be ready next week.\u003c/p\u003e","title":"Contracts are almost running","type":"blog"},{"content":"Last week was busy on various improvements for contracts infrastructure, trying to clarify existing API and ensuring everything that might be built is actually possible. First system contracts were introduced, trait support was added and more, below is a recap of key updates.\nEnvironment improvements changed the way calls into extension traits are made, statically guaranteeing that #[view] methods can\u0026rsquo;t recursively call methods that potentially #[update] slot contents, this is because #[view] methods are supposed to not modify anything and be callable from non-block context, so it\u0026rsquo;d be strange for them to be able to access API that is capable of altering persistent data. Now this is expressed on type system level.\n#[tmp] arguments were introduced as a bit of ephemeral state that only lives for a duration of a single transaction processing. This will allow to, for example, approve transfer of a specific amount of a specific token just for the duration of a single transaction and nothing else, which makes it possible to make the least privileged contract calls instead of allowing to do everything on behalf of the user by default. Also, safe contructors for VariableBytes and MaybeData were added that make calls into smart contracts more convenient.\nContacts overview (rendered) was added to the book with some diagrams detailing how storage is organized and \u0026ldquo;Everything is a contract\u0026rdquo; approach that is being tested right now. This was followed up in PR 19 and PR 27 with introduction of system contracts ab-system-contract-address-allocator, ab-system-contract-code and ab-system-contract-state that implement some core fundamental capabilities. This means that in contrast to older revisions of the code base \u0026ldquo;code\u0026rdquo; and \u0026ldquo;state\u0026rdquo; are no longer separate types of storage, they are just slots stored by system contracts, which happens to be known by the host so it can read/write those when needed. Address allocator is the contract which allocates addresses to contracts that are about to be deployed and is used by code contract during deployment of new contracts. There is still a lot more work left around system contracts, but so far the concept \u0026quot; Everything is a contract\u0026quot; seems to be working reasonably well.\nThere were some metadata improvements (PR 20, PR 21, PR 22) that massaged metadata information about the contract and its methods, followed by more #[contract] macro refactoring (PR 23, PR 24) that finally led to trait support being implemented. Trait support allows to define things like fungible token as a trait and for other contract to implement it. Then contracts can rely on just trait definition to be able to interact with any contract that has implemented that trait. Simple Fungible trait was added to the codebase just to demonstrate how it could work. More traits will be added over time, for example it is likely that some kind of \u0026ldquo;Wallet\u0026rdquo; trait will be defined to unify interation with contracts from the user side.\nThere were some smaller changes here and there as well, but if you\u0026rsquo;re interested in that you better go read numerous PRs directly instead.\nUpcoming plans # The next steps will involve implementing some kind of test environment for contract execution, such that it is possible to combine a couple of contracts, deploy them and see them interacting with each other. This will be an important milestone in showcasing developer experience and will hopefully help to collect some developer feedback.\n","date":"21 January 2025","externalUrl":null,"permalink":"/blog/2025-01-21-system-contracts-trait-support-and-more/","section":"Blog","summary":"\u003cp\u003eLast week was busy on various improvements for contracts infrastructure, trying to clarify existing API and ensuring\neverything that might be built is actually possible. First system contracts were introduced, trait support was added and\nmore, below is a recap of key updates.\u003c/p\u003e","title":"System contracts, trait support and more","type":"blog"},{"content":"Hello, world ðŸ‘‹!\nThis is the beginning of hopefully successful thing I call \u0026ldquo;Project Abundance\u0026rdquo;.\nAfter writing initial set of constraints and thinking about it for quite a while, it is finally time to dedicate all my time to it and see where it leads us.\nDuring last couple of weeks new repository was created with some initial CI infrastructure for building this website, a book and Rust docs of the code.\nA book was started that tries to provide some details about the project and will accumulate many technical details over time.\nSmart contracts initial dump was merged, which contains some infrastructure for building smart contracts that would run in (currently) imaginary execution environment. It is just an early prototype, but should give an idea of what things might look and feel.\nI have learned way too much about working writing Rust for const environment, about procedural macros and various static site generators.\nNext I\u0026rsquo;ll be thinking about how to add a notion of capabilities to smart contracts (imagine allowing to withdraw certain amount of certain token only) and documenting the architecture of execution environment with diagrams.\n","date":"13 January 2025","externalUrl":null,"permalink":"/blog/2025-01-13-welcome/","section":"Blog","summary":"\u003cp\u003eHello, world ðŸ‘‹!\u003c/p\u003e\n\u003cp\u003eThis is the beginning of hopefully successful thing I call \u0026ldquo;Project Abundance\u0026rdquo;.\u003c/p\u003e\n\u003cp\u003eAfter \u003ca href=\"https://gist.github.com/nazar-pc/760505c5ad7d56c20b2c75c1484e672f\" target=\"_blank\"\u003ewriting initial set of constraints\u003c/a\u003e and thinking about it for quite a while, it is finally time to dedicate all\nmy time to it and see where it leads us.\u003c/p\u003e","title":"Welcome!","type":"blog"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"categories","summary":"","title":"categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/series/","section":"series","summary":"","title":"series","type":"series"}]